<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>R4All on R4All</title>
    <link>/</link>
    <description>Recent content in R4All on R4All</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 15 Oct 2017 00:00:00 +0200</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title> </title>
      <link>/who-are-we/owen/</link>
      <pubDate>Wed, 20 Apr 2016 00:00:00 +0200</pubDate>
      
      <guid>/who-are-we/owen/</guid>
      <description></description>
    </item>
    
    <item>
      <title> </title>
      <link>/who-are-we/natalie/</link>
      <pubDate>Wed, 20 Apr 2016 00:00:00 +0200</pubDate>
      
      <guid>/who-are-we/natalie/</guid>
      <description></description>
    </item>
    
    <item>
      <title> </title>
      <link>/who-are-we/andrew/</link>
      <pubDate>Wed, 20 Apr 2016 00:00:00 +0200</pubDate>
      
      <guid>/who-are-we/andrew/</guid>
      <description></description>
    </item>
    
    <item>
      <title> </title>
      <link>/who-are-we/dylan/</link>
      <pubDate>Wed, 20 Apr 2016 00:00:00 +0200</pubDate>
      
      <guid>/who-are-we/dylan/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Maximum likelihood part 1</title>
      <link>/posts/maximum-likelihood-part-1/</link>
      <pubDate>Wed, 27 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>/posts/maximum-likelihood-part-1/</guid>
      <description>&lt;p&gt;This is the first in a series of posts about maximum likelihood methods for fitting statistical models to data. Inspiration for the material comes in large part from Drew Purves who presented something similar. Owen is using Drew’s approach as the basis for this course. Much of the R specific stuff is heavily influenced by Ben Bolker’s excellent book: Ecological Models and Data in R. The goal of this and the following posts includes:
learning how to fit to our data more mechanistic models of arbitrary complexity.
learning how to do this with ease, confidence, and complete transparency.
at some point delving into robust and efficient parameter estimation methods (e.g., MCMC).
at some point working out how to switch between frequentist and Bayesian approaches with ease.
(Please note that the focus of this post is learning about maximum likelihood methods. R is a only a tool to help that learning, so we avoid putting lots of potentially distracting R code in the post, and rather make it available as a separate file.)
Lets start on familiar ground by consider a common statistical approach, linear regression. This involves fitting a model (equation) to the observed data. The equation is:&lt;/p&gt;

&lt;p&gt;$$ y = a + bx + norm(0, \sigma) $$&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;$ y $ is the observed response variable&lt;/li&gt;
&lt;li&gt;$ x $ is the explanatory variable&lt;/li&gt;
&lt;li&gt;$ a $ is the intercept&lt;/li&gt;
&lt;li&gt;$ b $ is the slope&lt;/li&gt;
&lt;li&gt;$ norm(0, \sigma) $ is the error term.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This is the model we fit to our data. We are trying to find the value of $ a $, $ b $, and $ \sigma $ that fit the data. These are the parameters of the model. How do we find out the values of a and b that give the best regression line? Regression does so by minimizing the sum of squares, which is why we often use the term ‘least squares regression’. We are going to learn how to estimate the parameters of a model by maximising likelihood (instead of minimising least squares). We will start by considering a dataset and model even simpler than linear regression. This example may seem rather trivial. It is. However, better to start simple, with an example adequate to introduce many of the fundamental concepts we need.&lt;/p&gt;

&lt;p&gt;Assume we’ve counted the number of individuals in seven replicate quadrats that we’ve placed randomly in a field (don’t ask me why seven quadrats – maybe we left the other three in the lab – who knows). This is our observed data (seven numbers).&lt;/p&gt;

&lt;p&gt;Now, the definition of likelihood is $ p(data | model) $&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;$ p $ stands for probability&lt;/li&gt;
&lt;li&gt;* data * is the observed data&lt;/li&gt;
&lt;li&gt;$ | $ (the vertical bar) can be read as ‘given the’&lt;/li&gt;
&lt;li&gt;$ model $ = whatever model we like.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;That is, $ p(data | model) $ can be read as “the probability of the data given the model”.&lt;/p&gt;

&lt;p&gt;In our current example we will assume that individuals in the field are randomly distributed, and therefore that the count in each quadrat comes from a Poission distribution. The Poisson distribution has only one pa- rameter, the mean. This is what we want to estimate. Since this is a trivial example, we can easily find the most likely value of the mean by taking the arithmetic average (mean) of the observed values. We’re not go- ing to do this yet, since the purpose of using this example is illustration of concepts.&lt;/p&gt;

&lt;p&gt;So, our model is $ Y_i = Poisson(M) $, where $ Y_i $ is the ith (pronounced eye–eth) observation of the response variable (here the number of individuals counted in the first quadrat) and $ M $ is the mean of the Poisson distribution – this is what we want to estimate.&lt;/p&gt;

&lt;p&gt;We want to find the likelihood – $ p(data | model) $ – and so first we calculate the likelihood of each individual observation. The likelihood of the ith observation is,&lt;/p&gt;

&lt;p&gt;$$ p(Y_i | M) $$&lt;/p&gt;

&lt;p&gt;This is the probability of getting the value $ Y_i $ given the mean $ M $.
So if the first count $ Y_1 = 6 $ and we guess a mean of 7, we can find the probability of $ Y_1 = 6 $ in Excel with: &lt;code&gt;Poisson(6, 7, FALSE)&lt;/code&gt; (why false?). Or in R using &lt;code&gt;dpois(6, 7)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;If you do either of these you will find that the probability of observing a 6 given a Poisson distribution with mean 7 is about 0.15.&lt;/p&gt;

&lt;p&gt;We do this for each value of the response variable, log the probabilities, and then add them up.
Lets do this in Excel, using the worksheet ‘eg1’ in &lt;a href=&#34;/files/eg1.xlsx&#34; target=&#34;_blank&#34;&gt;this Excel spreadsheet&lt;/a&gt;. Here is a screen grab of the worksheet:&lt;/p&gt;




  

&lt;figure&gt;

&lt;img src=&#34;/img/excelgrab1.jpg&#34; /&gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  &lt;h4&gt;&lt;/h4&gt;
  
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The first column (from row 4 down) is the data, the second is the probability of each data point given the mean (look at the formula to see the call to the Excel Possion function). The third column is the log (base e; ln) of the probabilities (i.e., the log-likelihood. Often this is just called the likelihood, but we should try to always use the term log-likelihood, for accuracy. At the bottom of column C is the sum of the log-likelihoods. Cell A2 contains our guess of the mean of the observed data.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Look at the formulas in the cells to check they are what you expect.&lt;/li&gt;
&lt;li&gt;Try changing the guess of the mean and see what happens to the sum of the log probabilities.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;By changing the guess of the mean (cell A2 in the worksheet ‘eg1’) you change the log-likelihood. The challenge is to find the value of the mean that will minimise the log-likelihood (= maximise the likelihood). This value (estimate) of the mean is called the maximum likelihood estimate. We are interested in maximising the likelihood, or equivalently, minimising it (make as close to zero as possible). The more negative the sum of log likelihood the worse the model (guess of the mean) is. (Read this paragraph while playing with the Excel worksheet, until you are sure you get it. It is very important.)&lt;/p&gt;

&lt;p&gt;(By the way, we log the probabilities for a few reasons, including that computers often have problems deal- ing with very small numbers.)&lt;/p&gt;

&lt;p&gt;By playing with the guess of the mean in cell A2 of the Excel worksheet, you may have found that a value close to 6 minimises the log-likelihood (makes it closest to zero, i.e., least negative). You have just obtained a maximum likelihood estimate of the parameter of this model.&lt;/p&gt;

&lt;p&gt;Now take a look at the R script file &lt;a href=&#34;/files/eg1.r&#34; target=&#34;_blank&#34;&gt;eg1.r&lt;/a&gt;. This shows how you can do with R what you just did with Excel.&lt;/p&gt;

&lt;p&gt;Of course, we don’t want to have to guess the mean, look at the log-likelihood, guess again, and look at the log-likelihood again, and so on. We want the computer to do the work. In the R file is some script that makes a vector of guesses of the mean, loops through these, and records the log-likelihood. Then we can plot the value of the log-likelihood versus the guess of the mean.&lt;/p&gt;




  

&lt;figure&gt;

&lt;img src=&#34;/img/like_profile.jpg&#34; /&gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  &lt;h4&gt;&lt;/h4&gt;
  
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The maximum of this curve is the maximum likelihood. It is achieved when the guess of the mean has the val- ue of 6.1, where the vertical dashed line is drawn. The log-likelihood here is -14.821. This curve is also know as the likelihood profile.&lt;/p&gt;

&lt;p&gt;How did we do? Our guess was 6.1 and the actual mean is 6.143, with a log-likelihood of -14.820. Not bad then!
Question for you: What should we change in the R script to get a more accurate estimate of the mean?
Now, this is important. If we suspected our observed data were poisson distributed, and we wanted to mod- el that data in R, we might easily decide to use the generalised linear model funtion &lt;code&gt;glm&lt;/code&gt; and specify a pois- son distribution (family=poisson). The code to do this would look something like, &lt;code&gt;glm(n ~ 1, family=poisson)&lt;/code&gt; where n is the response variable that here is the number of individuals in each of the quadrats.&lt;/p&gt;

&lt;p&gt;This model is in the R script previously mentioned, and the coefficient (estimated parameter / mean) is 1.8153. Not the mean that we found (6.1). This is because specifying a poisson distribution (family=poisson) means that a log link function is used. So we have to un-log the coefficient to get the actual value&amp;hellip; exp(1.8153) = 6.1428. Good.&lt;/p&gt;

&lt;p&gt;What else can we do with this simple example? Let’s figure out for ourselves the AIC (Akaike Information Cri- teria) of our model. The definition of AIC is,&lt;/p&gt;

&lt;p&gt;$$ AIC = -2*L + 2*n $$&lt;/p&gt;

&lt;p&gt;Where L is the log-likelihood and p is the number of parameters (only one, the mean, in our model). Our log-likelihood was -14.82117, so AIC = 31.64. The AIC given by the &lt;code&gt;glm&lt;/code&gt; function is 31.64 also. Great.&lt;/p&gt;

&lt;p&gt;How does &lt;code&gt;glm&lt;/code&gt; work? How does it find the mean? It tries many guesses of the mean and sees which gives the maximum likelihood, just like we did in the R script that made the graph above. However, &lt;code&gt;glm&lt;/code&gt; uses a search method that is a bit smarter than ours (I think it calls the function &lt;code&gt;optim&lt;/code&gt; which by default uses the Nelder-Mead search algorith, but this is not too important now). What is important is to realise that &lt;code&gt;glm&lt;/code&gt; is just searching around the parameter space looking for a maximum likelihood. It tries to go uphill on the likelihood profile. When it can’t go uphill any more, it knows it’s reached the maximum likelihood.
You should now be able to have a good go at answering these questions:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;What are parameters?&lt;/li&gt;
&lt;li&gt;What is a variable?&lt;/li&gt;
&lt;li&gt;What is likelihood?&lt;/li&gt;
&lt;li&gt;What does this mean p(data| model)?&lt;/li&gt;
&lt;li&gt;How can we maximise likelihood.&lt;/li&gt;
&lt;li&gt;What is the maximum likelihood estimate of a parameter?&lt;/li&gt;
&lt;li&gt;What is a likelihood profile?&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Why adjust your r-squared</title>
      <link>/posts/why-adjust-your-r-squared/</link>
      <pubDate>Wed, 27 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>/posts/why-adjust-your-r-squared/</guid>
      <description>&lt;p&gt;Just a little demo of what happens if you don’t or do adjust your r-squared. Here’s the bottom line&amp;hellip;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/posts/2019-02-27-why-adjust-your-r-squared_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;

&lt;p&gt;As we increase the number of explanatory variables in a linear model (e.g. multiple regression) the unadjusted r-squared increaes (green dots) even if the additional explanatory variables contain only random numbers. The adjusted r-squared is &amp;ldquo;adjusted&amp;rdquo; so it does not! So if we simply want to know the proportion of variance explained by our model we are fine using the unadjusted r-squared. If, however, we want to compare the r-squared of models with different numbers of explanatory variables, we should compare the adjusted r-squared.&lt;/p&gt;

&lt;p&gt;Here&amp;rsquo;s the code for making the figure. (Done before we converted to the tidyverse!)&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## Lets do some multiple regression, with different numbers of explanatory variables
## with completely random data
numb.expl.vars &amp;lt;- floor(rep(2^seq(0, 5, 0.5), each=50))

## Number of observations
n &amp;lt;- 100

## The response variable
y &amp;lt;- rnorm(n)

## Function to return the unadjusted and adjusted r-squared
get.r2 &amp;lt;- function(ne) {
  x &amp;lt;- as.data.frame(matrix(rnorm(n*ne), n, ne))
  m1 &amp;lt;- lm(y ~ ., x)
  result &amp;lt;- c(summary(m1)$r.squared, summary(m1)$adj.r.squared)
  result
}

## use lapply to run the function over the number of explanatory variables vec
rez &amp;lt;- do.call(rbind, lapply(numb.expl.vars, function(x) get.r2(x)))

## get the mean r-squared and adjusted r-squared per number of expl varbs
means &amp;lt;- aggregate(rez, list(numb.expl.vars=numb.expl.vars), mean)

## plot the data
matplot(log2(numb.expl.vars), rez, type=&amp;quot;n&amp;quot;, ann=F, axes=F)
box()
abline(h=0)
matpoints(jitter(log2(numb.expl.vars)), rez, pch=19, col=c(&amp;quot;#11ff1144&amp;quot;, &amp;quot;#ff111144&amp;quot;))
mtext(1, line=2.5, text=&amp;quot;Number of explanatory variables&amp;quot;)
mtext(2, line=2, text=&amp;quot;R-squared\n(green unadjusted, red adjusted)&amp;quot;)
axis(1, at=0:5, labels=2^(0:5))
axis(2)
matpoints(log2(means[,1]), means[,2], pch=21, bg=c(&amp;quot;#11ff1144&amp;quot;))
matpoints(log2(means[,1]), means[,3], pch=21, bg=c(&amp;quot;#ff111144&amp;quot;))

## for fun, calculate the adjusted r-squared manually
adj.rsquared &amp;lt;- 1 - (1-rez[,1])*(n-1)/(n-numb.expl.vars-1)
sum(abs(adj.rsquared-rez[,2])&amp;gt;1e-10) ## should be zero
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Getting lost packages</title>
      <link>/posts/getting-lost-packages/</link>
      <pubDate>Tue, 05 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>/posts/getting-lost-packages/</guid>
      <description>&lt;p&gt;To use the functions in an add-on package you first need to install the package. Remember you only need install it once.&lt;/p&gt;

&lt;p&gt;During the writing of the book, and in early 2018 the normal method for installing the &lt;strong&gt;ggfortify&lt;/strong&gt; add-on package didn’t work (we got the message &lt;code&gt;package ggfortify is not available (for R Version 3.2.4)&lt;/code&gt;).&lt;/p&gt;

&lt;p&gt;This has not happened for some time, so hopefully you won&amp;rsquo;t experience it. If you do&amp;hellip;&lt;/p&gt;

&lt;p&gt;Many packages are now developed in an environment / web service called &lt;em&gt;GitHub&lt;/em&gt;, and R has an interface for installing packages directly from &lt;em&gt;GitHub&lt;/em&gt;. This is true for &lt;em&gt;ggfortify&lt;/em&gt;. Here is how you can access the package that way:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;install.packages(&amp;quot;devtools&amp;quot;)
library(devtools)
install_github(&#39;sinhrks/ggfortify&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Another option: if the packages was once on CRAN, and archived, older version may still be available. Probably you can find this by googling ggfortify cran and following links to a rather unfriendly looking web page that lists a few files, one of which was, at the time of writing this, gfortify_0.1.0.tar.gz. Download this to your computer, making sure you know which folder it gets downloaded into. Then, in Rstudio, Click the Install button in the Packages (as usual when you want to install a package). However, instead of Installing from: Repository CRAN choose Install from: Package Archive File (.tgz, .tar.gz). Then you need to click Browse and find the file you just downloaded. Then click Install. Hopefully it will still work with the version of R you have installed.&lt;/p&gt;

&lt;p&gt;Another option is to do it the old fashioned way. E.g. instead of using the autoplot() function from the ggfortify package, do this:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;par(mfrow(2,2))
plot(model, add.smooth = FALSE)
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Importing data update</title>
      <link>/posts/importing-data-update/</link>
      <pubDate>Tue, 05 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>/posts/importing-data-update/</guid>
      <description>&lt;p&gt;As the Second Edition of &lt;em&gt;Getting Started with R&lt;/em&gt; was going to press, Rstudio changed the function it uses to import data in the &lt;strong&gt;Import Dataset&lt;/strong&gt; tool, from the base function &lt;code&gt;read.csv()&lt;/code&gt; to the &lt;code&gt;read_csv()&lt;/code&gt; function in the &lt;strong&gt;readr&lt;/strong&gt; package. Since then, the &lt;strong&gt;Import Dataset&lt;/strong&gt; button gives a menu with an option to use either (&amp;ldquo;base&amp;rdquo; uses &lt;code&gt;read.csv&lt;/code&gt; and &amp;ldquo;readr&amp;rdquo; uses &lt;code&gt;read_csv&lt;/code&gt;) From the Rstudio Blog about the &lt;strong&gt;readr&lt;/strong&gt; package:&lt;/p&gt;

&lt;p&gt;Compared to base equivalents like &lt;code&gt;read.csv&lt;/code&gt;, readr is much faster and gives more convenient output: it never converts strings to factors, can parse date/times, and it doesn’t munge the column names.&lt;/p&gt;

&lt;p&gt;Great! If you have dates and times in a column, its possible that &lt;code&gt;read_csv&lt;/code&gt; will see this and then correctly format them as dates. So you wouldn’t need to do this yourself (covered in Appendix of Chapter 2 of the Second Edition).&lt;/p&gt;

&lt;p&gt;Something else you might notice is that the data appears different if you look at it in the Console. This is because &lt;code&gt;read_csv&lt;/code&gt; brings the data in as a special type of object called a &lt;em&gt;tibble&lt;/em&gt;. (The standard &lt;code&gt;read.csv&lt;/code&gt; function bring the data in as a standard data.frame.) You can read all about tibbles on the RStudio blog. Why are they better than data.frames? First, they only show the first ten lines and as many variables as will comfortably fit when you look at them in the Console, and the type of variable is given, and the number of additional rows and variables not displayed. E.g.:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(readr)
dd &amp;lt;- read_csv(readr_example(&amp;quot;mtcars.csv&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## Parsed with column specification:
## cols(
##   mpg = col_double(),
##   cyl = col_double(),
##   disp = col_double(),
##   hp = col_double(),
##   drat = col_double(),
##   wt = col_double(),
##   qsec = col_double(),
##   vs = col_double(),
##   am = col_double(),
##   gear = col_double(),
##   carb = col_double()
## )
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;dd
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## # A tibble: 32 x 11
##      mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb
##    &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
##  1  21       6  160    110  3.9   2.62  16.5     0     1     4     4
##  2  21       6  160    110  3.9   2.88  17.0     0     1     4     4
##  3  22.8     4  108     93  3.85  2.32  18.6     1     1     4     1
##  4  21.4     6  258    110  3.08  3.22  19.4     1     0     3     1
##  5  18.7     8  360    175  3.15  3.44  17.0     0     0     3     2
##  6  18.1     6  225    105  2.76  3.46  20.2     1     0     3     1
##  7  14.3     8  360    245  3.21  3.57  15.8     0     0     3     4
##  8  24.4     4  147.    62  3.69  3.19  20       1     0     4     2
##  9  22.8     4  141.    95  3.92  3.15  22.9     1     0     4     2
## 10  19.2     6  168.   123  3.92  3.44  18.3     1     0     4     4
## # … with 22 more rows
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In summary, everything in the Second Edition will work just fine, but what you see in the Console might look a little different at times.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Datasets</title>
      <link>/books/datasets/</link>
      <pubDate>Sun, 09 Sep 2018 00:00:00 +0200</pubDate>
      
      <guid>/books/datasets/</guid>
      <description>

&lt;h2 id=&#34;datasets-used-in-getting-started-with-r-second-edition&#34;&gt;Datasets used in &lt;em&gt;Getting Started with R&lt;/em&gt;, Second Edition&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/R4All/datasets/archive/master.zip&#34;&gt;All the datasets (and more) one zip file&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Datasets for the first edition of &lt;em&gt;Getting Started with R&lt;/em&gt; are also contained in that zip file.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Getting Started with R, Second Edition</title>
      <link>/books/gswr2/</link>
      <pubDate>Sun, 09 Sep 2018 00:00:00 +0200</pubDate>
      
      <guid>/books/gswr2/</guid>
      <description>




  

&lt;figure&gt;

&lt;img src=&#34;/img/SecondEdCover.png&#34; /&gt;


&lt;/figure&gt;

&lt;p&gt;We are pleased to announce the second edition of Getting Started with R
Paperback at &lt;a href=&#34;https://global.oup.com/academic/product/getting-started-with-r-9780198787846?cc=ch&amp;amp;lang=en&amp;amp;&#34;&gt;OUP&lt;/a&gt;, &lt;a href=&#34;https://www.amazon.co.uk/Getting-Started-R-Introduction-Biologists/dp/0198787847&#34;&gt;Amazon UK&lt;/a&gt;, &lt;a href=&#34;https://www.amazon.com/Getting-Started-R-Introduction-Biologists/dp/0198787847&#34;&gt;Amazon.com&lt;/a&gt;, &lt;a href=&#34;https://www.amazon.de/Getting-Started-R-Andrew-Beckerman/dp/0198787847&#34;&gt;Amazon.de&lt;/a&gt;, &lt;a href=&#34;https://www.orellfuessli.ch/shop/home/suchartikel/ID46438190.html&#34;&gt;Buch.ch&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://www.oxfordscholarship.com/view/10.1093/acprof:oso/9780198787839.001.0001/acprof-9780198787839&#34;&gt;Electric version at Oxford Scholarship Online.&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;We got great feedback about our book &amp;ldquo;Getting Started with R, An Introduction for Biologists&amp;rdquo;, but realised over the last couple of years that we were using and teaching R very differently from what we wrote about back in 2010. So we have almost completely rewritten for a very different and new Second Edition.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Now based on using RStudio&lt;/li&gt;
&lt;li&gt;Now based around dplyr and ggplot2 for data management and grahics&lt;/li&gt;
&lt;li&gt;New basic examples for regression, 1-way ANOVA and 2-way ANOVA&lt;/li&gt;
&lt;li&gt;A whole new chapter on the Generalised Linear Model&lt;/li&gt;
&lt;li&gt;That same Getting Started with R attitude.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;While many of the tools and functions have changed in this new edition, the content remains clear and enjoyable. We still walk your through, in that R4All style, from the basics of setting up your computer, through to data import and exploration and ultimately to a series of statistical models for interpreting data and experiments.&lt;/p&gt;

&lt;h2 id=&#34;what-is-so-different-and-new&#34;&gt;What is so different and new?&lt;/h2&gt;

&lt;p&gt;Several recent developments in the R community have made it even easier for us to help you focus more on your data and questions. RStudio has emerged as a brilliant cross-platform interface to working with R. Furthermore, Hadley Wickham and colleagues, have developed several add-on packages including as dplyr, tidyr and ggplot2 that not only provide consistent and intuitive ways to work with your data, but are emerging as industry standards (academic and non-academic).&lt;/p&gt;

&lt;p&gt;We couldn&amp;rsquo;t afford to ignore these developments. The second edition thus dispenses entirely with classic methods of using R, and instead embraces RStudio along with dplyr, tidyr and ggplot2.&lt;/p&gt;

&lt;p&gt;You&amp;rsquo;ll love it.&lt;/p&gt;

&lt;h2 id=&#34;anything-else-new&#34;&gt;Anything else new?&lt;/h2&gt;

&lt;p&gt;We have a new co-author, Dylan Childs. He better at using R than Owen and Andrew, and kind of grumpy, so fits in well.  He&amp;rsquo;s also helped us expand our stats offerings in the book.  Not only do we provide more examples for basic stats (Regression, 1-way ANOVA, 2-way ANOVA) but a whole new chapter on the Generalised Linear Model.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Miscellany</title>
      <link>/books/miscellany/</link>
      <pubDate>Sun, 09 Sep 2018 00:00:00 +0200</pubDate>
      
      <guid>/books/miscellany/</guid>
      <description>

&lt;h2 id=&#34;lost-packages&#34;&gt;Lost packages&lt;/h2&gt;

&lt;p&gt;Here is a post &lt;a href=&#34;/posts/getting-lost-packages/&#34;&gt;Lost Packages&lt;/a&gt; about what to do if, when attempting to install a package you get a message like &lt;code&gt;package ggfortify is not available (for R Version 3.2.4)&lt;/code&gt;.&lt;/p&gt;

&lt;h2 id=&#34;importing-data-update&#34;&gt;Importing data update&lt;/h2&gt;

&lt;p&gt;If you&amp;rsquo;re having a brain melt figuring out the difference between &lt;code&gt;read_csv&lt;/code&gt; and &lt;code&gt;read.csv&lt;/code&gt; and their use in the RStudio Import Dataset tool, look at this post about &lt;a href=&#34;/posts/importing-data-update/&#34;&gt;using the &lt;strong&gt;readr&lt;/strong&gt; package and &lt;code&gt;read_csv&lt;/code&gt; function to import data.&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;about-the-first-edition-of-getting-started-with-r&#34;&gt;About the first edition of &lt;em&gt;Getting Started with R&lt;/em&gt;&lt;/h2&gt;




  

&lt;figure&gt;

&lt;img src=&#34;/img/FirstEdCover.jpg&#34; /&gt;


&lt;/figure&gt;

&lt;p&gt;“Blowing away any feeling of intimidation is what this book is about.” – &lt;a href=&#34;https://www.sciencedirect.com/science/article/pii/S0169534712001590&#34;&gt;Graeme Ruxton, Trends in Ecology Evolution&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Designed for undergraduates, grad students and staff, we show you how to import, explore, graph, and start analysing your data, keeping you focused on your ultimate goals.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;A simple, easy, and engaging introduction to R for biologists.&lt;/li&gt;
&lt;li&gt;Walks readers through the fundamentals of using R, from that first step of importing datathrough to managing and exploring it and ultimately producing figures and analyses&lt;/li&gt;
&lt;li&gt;Delivers an efficient, accurate, reliable, and reproducible workflow.&lt;/li&gt;
&lt;li&gt;We provide a simple, efficient, reliable, accurate, and reproducible workflow for you in an engaging and sometimes humorous format.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Getting Started with R – its the book you and your students want to read before all the others.&lt;/p&gt;

&lt;p&gt;Well, it was. Now you should only look at &lt;a href=&#34;/books/gswr2/&#34;&gt;the Second Edition&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
