<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>R4All on R4All</title>
    <link>https://r4all.org/</link>
    <description>Recent content in R4All on R4All</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 15 Oct 2017 00:00:00 +0200</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title> </title>
      <link>https://r4all.org/who-are-we/owen/</link>
      <pubDate>Wed, 20 Apr 2016 00:00:00 +0200</pubDate>
      
      <guid>https://r4all.org/who-are-we/owen/</guid>
      <description></description>
    </item>
    
    <item>
      <title> </title>
      <link>https://r4all.org/who-are-we/natalie/</link>
      <pubDate>Wed, 20 Apr 2016 00:00:00 +0200</pubDate>
      
      <guid>https://r4all.org/who-are-we/natalie/</guid>
      <description></description>
    </item>
    
    <item>
      <title> </title>
      <link>https://r4all.org/who-are-we/andrew/</link>
      <pubDate>Wed, 20 Apr 2016 00:00:00 +0200</pubDate>
      
      <guid>https://r4all.org/who-are-we/andrew/</guid>
      <description></description>
    </item>
    
    <item>
      <title> </title>
      <link>https://r4all.org/who-are-we/dylan/</link>
      <pubDate>Wed, 20 Apr 2016 00:00:00 +0200</pubDate>
      
      <guid>https://r4all.org/who-are-we/dylan/</guid>
      <description></description>
    </item>
    
    <item>
      <title>levels() not working in 2nd Edition of Getting Started</title>
      <link>https://r4all.org/posts/2022-09-10-levels-not-working-in-2nd-edition-of-getting-started/</link>
      <pubDate>Sat, 10 Sep 2022 00:00:00 +0000</pubDate>
      
      <guid>https://r4all.org/posts/2022-09-10-levels-not-working-in-2nd-edition-of-getting-started/</guid>
      <description>
&lt;script src=&#34;https://r4all.org/posts/2022-09-10-levels-not-working-in-2nd-edition-of-getting-started/index.en_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;Sometimes, in the second edition, we use the &lt;code&gt;levels&lt;/code&gt; function to get the unique levels of a variable. For example on page 133 we do &lt;code&gt;levels(growth.moo$diet)&lt;/code&gt; to get the unique levels of the &lt;code&gt;diet&lt;/code&gt; variable. Today, this does not work. Below I explain why and how to fix it. Short version is use &lt;code&gt;unique&lt;/code&gt; instead of &lt;code&gt;levels&lt;/code&gt; or convert the variables to factors.&lt;/p&gt;
&lt;div id=&#34;prepare&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Prepare&lt;/h2&gt;
&lt;p&gt;We will use the &lt;code&gt;mutate&lt;/code&gt; function from the &lt;code&gt;dplyr&lt;/code&gt; package, so please ensure you have that package installed.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;import-the-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Import the data&lt;/h2&gt;
&lt;p&gt;In the next line of code I import the data from github, rather than a local copy. This saves us having to deal with local location of the data file. I would normally work with a local copy, however.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;growth.moo &amp;lt;- read.csv(url(&amp;quot;https://raw.githubusercontent.com/r4all/datasets/master/growth.csv&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;using-unique-rather-than-levels&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Using &lt;code&gt;unique&lt;/code&gt; rather than &lt;code&gt;levels&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;Looking at the structure of the data in R we see:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str(growth.moo)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;#39;data.frame&amp;#39;:    48 obs. of  3 variables:
##  $ supplement: chr  &amp;quot;supergain&amp;quot; &amp;quot;supergain&amp;quot; &amp;quot;supergain&amp;quot; &amp;quot;supergain&amp;quot; ...
##  $ diet      : chr  &amp;quot;wheat&amp;quot; &amp;quot;wheat&amp;quot; &amp;quot;wheat&amp;quot; &amp;quot;wheat&amp;quot; ...
##  $ gain      : num  17.4 16.8 18.1 15.8 17.7 ...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Supplement and diet are both &lt;code&gt;chr&lt;/code&gt; (character) type variables.&lt;/p&gt;
&lt;p&gt;Hence the &lt;code&gt;levels&lt;/code&gt; function doesn’t give us the levels. Instead, we get &lt;code&gt;NULL&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;levels(growth.moo$supplement)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## NULL&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;levels(growth.moo$diet)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## NULL&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So, instead use &lt;code&gt;unique&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;unique(growth.moo$supplement)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;supergain&amp;quot; &amp;quot;control&amp;quot;   &amp;quot;supersupp&amp;quot; &amp;quot;agrimore&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;unique(growth.moo$diet)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;wheat&amp;quot;  &amp;quot;oats&amp;quot;   &amp;quot;barley&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Awesomeness!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;converting-to-a-factor&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Converting to a factor&lt;/h2&gt;
&lt;p&gt;Another option is to convert the &lt;code&gt;chr&lt;/code&gt; type variables to be factor type variables. There are many ways to achieve this, here are two.&lt;/p&gt;
&lt;p&gt;If we want to convert to factors all the &lt;code&gt;chr&lt;/code&gt; variables in our data, then we can use the &lt;code&gt;type.convert&lt;/code&gt; function with the argument &lt;code&gt;as.is = FALSE&lt;/code&gt;. Making this &lt;code&gt;FALSE&lt;/code&gt; tells the &lt;code&gt;type.convert&lt;/code&gt; function to &lt;em&gt;not&lt;/em&gt; keep character type variables &lt;em&gt;as they are&lt;/em&gt;, but rather to convert them to factors.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;growth.moo.factors1 &amp;lt;- type.convert(growth.moo, as.is = FALSE)
str(growth.moo.factors1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;#39;data.frame&amp;#39;:    48 obs. of  3 variables:
##  $ supplement: Factor w/ 4 levels &amp;quot;agrimore&amp;quot;,&amp;quot;control&amp;quot;,..: 3 3 3 3 2 2 2 2 4 4 ...
##  $ diet      : Factor w/ 3 levels &amp;quot;barley&amp;quot;,&amp;quot;oats&amp;quot;,..: 3 3 3 3 3 3 3 3 3 3 ...
##  $ gain      : num  17.4 16.8 18.1 15.8 17.7 ...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Great! What were character type variables are now factors.&lt;/p&gt;
&lt;p&gt;By the way, the default since R 4.0.0 is &lt;code&gt;as.is = TRUE&lt;/code&gt; which can be understood as keep variables as they are–do not convert them to factors. We wrote the second edition before 4.0.0, and this is why &lt;code&gt;levels&lt;/code&gt; worked when we wrote the second edition, but does not work now.&lt;/p&gt;
&lt;p&gt;Another way is to individually convert each variable, for example:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;growth.moo.factors2  &amp;lt;- dplyr::mutate(growth.moo,
                                      supplement = as.factor(supplement),
                                      diet = as.factor(diet))
str(growth.moo.factors2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;#39;data.frame&amp;#39;:    48 obs. of  3 variables:
##  $ supplement: Factor w/ 4 levels &amp;quot;agrimore&amp;quot;,&amp;quot;control&amp;quot;,..: 3 3 3 3 2 2 2 2 4 4 ...
##  $ diet      : Factor w/ 3 levels &amp;quot;barley&amp;quot;,&amp;quot;oats&amp;quot;,..: 3 3 3 3 3 3 3 3 3 3 ...
##  $ gain      : num  17.4 16.8 18.1 15.8 17.7 ...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Awesomeness 2!&lt;/p&gt;
&lt;p&gt;Thanks for reading. Have a nice day!&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>How we teach ggplot (in the Insights book)</title>
      <link>https://r4all.org/posts/2020-12-16-how-we-teach-ggplot-in-insights/</link>
      <pubDate>Wed, 16 Dec 2020 00:00:00 +0000</pubDate>
      
      <guid>https://r4all.org/posts/2020-12-16-how-we-teach-ggplot-in-insights/</guid>
      <description>&lt;p&gt;In our book &lt;a href=&#34;https://r4all.org/books/insights/&#34;&gt;&lt;em&gt;Insights&lt;/em&gt;&lt;/a&gt; we help readers learn how to use the amazing &lt;strong&gt;ggplot2&lt;/strong&gt; package to make visualisations. If you have some experience with &lt;strong&gt;ggplot2&lt;/strong&gt;, you may think our method of teaching it, and of using it in the book are a bit odd. Here we explain our reason for teaching it the way we teach it.&lt;/p&gt;

&lt;p&gt;For example, here is the code for the first graph we make:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;bats_Age_Sex %&amp;gt;%
  ggplot() +
    geom_col(mapping = aes(x=Sex, y=num_bat_IDs, fill=Age))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And if we had not piped the dataset into &lt;em&gt;ggplot&lt;/em&gt; then we would have done this:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ggplot() +
  geom_col(data = bats_Age_Sex,
           mapping = aes(x=Sex, y=num_bat_IDs, fill=Age))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Generally speaking, we first teach students to put all the arguments into the &lt;em&gt;geom&lt;/em&gt;. We believe this is a valuable didactic tool/approach (and also not bad to do in any case). This is because we believe that it&amp;rsquo;s important to know how the &lt;em&gt;geoms&lt;/em&gt; work and what they need, and that this is best seen by specifying the necessary information in the &lt;em&gt;geom&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;After we&amp;rsquo;ve explained and practiced this approach numerous times with different &lt;em&gt;geoms&lt;/em&gt;, we explain how inheritance from the &lt;em&gt;ggplot&lt;/em&gt; function works, and when it is useful.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Insights from Data with R.</title>
      <link>https://r4all.org/books/insights/</link>
      <pubDate>Wed, 16 Dec 2020 00:00:00 +0100</pubDate>
      
      <guid>https://r4all.org/books/insights/</guid>
      <description>&lt;p&gt;&lt;em&gt;An Introduction for the Life and Environmental Sciences.&lt;/em&gt;&lt;/p&gt;




  

&lt;figure&gt;

&lt;img src=&#34;https://r4all.org/img/InsightsCoverSmall.png&#34; /&gt;


&lt;/figure&gt;

&lt;p&gt;We are pleased to announce our brand new book, &lt;em&gt;Insights from Data with R, An Introduction for the Life and Environmental Sciences.&lt;/em&gt; We call it &lt;em&gt;Insights&lt;/em&gt; for short.&lt;/p&gt;

&lt;p&gt;It is published by OUP and available from the &lt;a href=&#34;https://global.oup.com/academic/product/insights-from-data-with-r-9780198849827&#34;&gt;OUP website&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Table of contents&lt;/strong&gt;: Here is the &lt;a href=&#34;https://r4all.org/files/Insights_ToC.pdf&#34; target=&#34;_blank&#34;&gt;Table of Contents (a pdf)&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Preface&lt;/strong&gt;: Here is the &lt;a href=&#34;https://r4all.org/files/Insights_Preface.pdf&#34; target=&#34;_blank&#34;&gt;Preface section (a  pdf)&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Insights&lt;/em&gt; is designed and written for undergraduates in the life and environmental sciences. It assumes no knowledge of statistics, data or programming.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Insights&lt;/em&gt; is intended as a go-to resource to accompany undergraduates during their first experience of working with data to get answers.&lt;/p&gt;

&lt;p&gt;Unique/important features of &lt;em&gt;Insights&lt;/em&gt; include:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Based on very well received undergraduate data analysis courses taught by the authors.&lt;/li&gt;
&lt;li&gt;Students work along with data from real scientific studies published on data repositories.&lt;/li&gt;
&lt;li&gt;Has four detailed &lt;em&gt;Workflow Demonstrations&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;Focuses on developing solid foundations of data management, summarising, and visualisation, to get robust and reproducible insights.&lt;/li&gt;
&lt;li&gt;Intentionally leaves statistical tests to subsequent courses/books.&lt;/li&gt;
&lt;li&gt;Uses RStudio and the &lt;em&gt;tidyverse&lt;/em&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For more information about the book, please check out the &lt;a href=&#34;http://insightsfromdata.io/&#34;&gt;companion website&lt;/a&gt;, where there are extensive additional resources (including three of the four Workflow Demonstrations).&lt;/p&gt;

&lt;p&gt;Our other book &lt;em&gt;Getting Started with R, An Introduction for Biologists&lt;/em&gt; is for people that have already some experience with data analysis, and that want to learn or improve their R competence and confidence.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Diagnostic plots of linear models with categotical explanatory variables</title>
      <link>https://r4all.org/posts/diagnostic-plots-of-models-with-categotical-explanatory-variables/</link>
      <pubDate>Thu, 19 Mar 2020 00:00:00 +0000</pubDate>
      
      <guid>https://r4all.org/posts/diagnostic-plots-of-models-with-categotical-explanatory-variables/</guid>
      <description>&lt;p&gt;[This is a minimal post due to very limited time.]&lt;/p&gt;

&lt;p&gt;We need to check the assumptions of our linear model (e.g. regression, ANOVA, ANCOVA) are not too badly violated. We often use four diagnostic graphs to do so. One of these shows standardised residuals plotted against leverage (each observation has a value).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The take home message of this post is&lt;/strong&gt; if your model contains at least one continuous explanatory variable, use the base R methods for making your diagnostic plots:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;par(mfrow = c(2,2))
plot(my_model)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Here is an example that looks at relationship between earthworm mass (response variable) and two explanatory variables (species ID of the earthworm, and stomach circumference of the earthworm).&lt;/p&gt;

&lt;p&gt;Prepare R:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(tidyverse)
library(ggfortify)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Read the data and make English variable names:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;worms &amp;lt;- read_csv(&amp;quot;https://raw.githubusercontent.com/opetchey/BIO144/master/3_datasets/earthworm.csv&amp;quot;) %&amp;gt;%
  rename(Mass = Gewicht,
         Species = Gattung,
         Stomach_circum = Magenumf)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## Parsed with column specification:
## cols(
##   Gattung = col_character(),
##   Nummer = col_double(),
##   Gewicht = col_double(),
##   Fangdatum = col_character(),
##   Magenumf = col_double()
## )
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Plot and inspect the data:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;worms %&amp;gt;%
  ggplot() +
  geom_point(mapping = aes(x = Stomach_circum, y = Mass, col = Species))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://r4all.org/posts/2020-03-19-diagnostic-plots-of-models-with-categotical-explanatory-variables_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;

&lt;p&gt;From this, we expect to see evidence of non-linearity in the diagnostic plot of residuals against fitted values (but this does not concern the issue addressed in this post).&lt;/p&gt;

&lt;p&gt;Now make the model including both explanatory variables and no interaction between them:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;mod_sp_circ_noint &amp;lt;- lm(Mass ~ Stomach_circum + Species, data = worms)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Here is the base R method for making the four model diagnostic plots:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;par(mfrow=c(2,2))
plot(mod_sp_circ_noint)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://r4all.org/posts/2020-03-19-diagnostic-plots-of-models-with-categotical-explanatory-variables_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Take note of the plot of standardised residuals versus leverage.&lt;/p&gt;

&lt;p&gt;Now compare to the same produced by the &lt;code&gt;autoplot&lt;/code&gt; function of the &lt;strong&gt;ggfortify&lt;/strong&gt; package:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;autoplot(mod_sp_circ_noint)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://r4all.org/posts/2020-03-19-diagnostic-plots-of-models-with-categotical-explanatory-variables_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;

&lt;p&gt;It seems that the &lt;code&gt;autoplot&lt;/code&gt; function in the &lt;strong&gt;ggfortify&lt;/strong&gt; package is not doing what we would like and expect&amp;hellip; there is a continuous explanatory variable, so leverage is not constant, and it should make a graph with leverage on the x-axis.&lt;/p&gt;

&lt;p&gt;Compare this difference in behaviour between base R and &lt;code&gt;ggfortify::autoplot&lt;/code&gt; when there is only a continuous explanatory variable in the model:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;mod_circ &amp;lt;- lm(Mass ~ Stomach_circum, data = worms)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;par(mfrow=c(2,2))
plot(mod_circ)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://r4all.org/posts/2020-03-19-diagnostic-plots-of-models-with-categotical-explanatory-variables_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;autoplot(mod_circ)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://r4all.org/posts/2020-03-19-diagnostic-plots-of-models-with-categotical-explanatory-variables_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;

&lt;p&gt;And when there is only a factor variable:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;mod_spp &amp;lt;- lm(Mass ~ Species, data = worms)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;par(mfrow=c(2,2))
plot(mod_spp)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://r4all.org/posts/2020-03-19-diagnostic-plots-of-models-with-categotical-explanatory-variables_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;autoplot(mod_spp)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://r4all.org/posts/2020-03-19-diagnostic-plots-of-models-with-categotical-explanatory-variables_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;

&lt;p&gt;I believe it is useful to have the residuals versus leverage plot if there is continuous explanatory variable, so would then use the base R method to make the diagnostic plots.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Lurking variables and hidden relationships</title>
      <link>https://r4all.org/posts/lurking-variables-and-hidden-relationships/</link>
      <pubDate>Mon, 16 Mar 2020 00:00:00 +0000</pubDate>
      
      <guid>https://r4all.org/posts/lurking-variables-and-hidden-relationships/</guid>
      <description>&lt;p&gt;Inspiration for the following from from Richard McElreath&amp;rsquo;s Statistical Rethinking book, and some of the code comes from here: &lt;a href=&#34;https://bookdown.org/ajkurz/Statistical_Rethinking_recoded/multivariate-linear-models.html#masked-relationship&#34;&gt;https://bookdown.org/ajkurz/Statistical_Rethinking_recoded/multivariate-linear-models.html#masked-relationship&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Let us think about the question of how the response variable &lt;em&gt;y&lt;/em&gt; is related to two explanatory variables &lt;em&gt;x1&lt;/em&gt; and &lt;em&gt;x2&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;First we make a dataset in which we know the relationships because we specify them: we make &lt;em&gt;y = x1 - x2&lt;/em&gt;. Before this, we create &lt;em&gt;x1&lt;/em&gt; and &lt;em&gt;x2&lt;/em&gt; and make them correlated&amp;hellip;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;rm(list=ls())
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(tidyverse)
library(patchwork)
set.seed(141)  # setting the seed makes the results reproducible
N   &amp;lt;- 100   # number of observations
rho &amp;lt;- .8      # correlation between x_pos and x_neg
dd &amp;lt;- 
  tibble(x1 = rnorm(N),                            
         x2 = rnorm(N, rho*x1, sqrt(1 - rho^2)),  
         y     = rnorm(N, x1 - x2))               
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;A quick look at the dataset&amp;hellip; three numeric &lt;code&gt;&amp;lt;dbl&amp;gt;&lt;/code&gt; variables.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;glimpse(dd)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## Observations: 100
## Variables: 3
## $ x1 &amp;lt;dbl&amp;gt; 0.51435972, -0.11277738, 0.06434006, -0.65524480, 0.50172420, -0.8…
## $ x2 &amp;lt;dbl&amp;gt; 0.929913616, 0.824215658, -0.060507248, -0.182433894, 1.952874568,…
## $ y  &amp;lt;dbl&amp;gt; -1.927385720, -0.028922502, 0.405574747, 0.629677348, -2.459065811…
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Figure 1 shows the three scatterplots on can make. Importantly, we see little evidence of the relationship between &lt;em&gt;y&lt;/em&gt; and &lt;em&gt;x1&lt;/em&gt;, or between &lt;em&gt;y&lt;/em&gt; and &lt;em&gt;x2&lt;/em&gt; that we know exist. We can clearly see the correlation between the two explanatory variables &lt;em&gt;x1&lt;/em&gt; and &lt;em&gt;x2&lt;/em&gt;.&lt;/p&gt;

&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;
&lt;img src=&#34;https://r4all.org/posts/2020-03-16-lurking-variables-and-hidden-relationships_files/figure-html/masked-rels-1-1.png&#34; alt=&#34;(a and b) Little evidence of a relationship between the response variable *y* and either of the two explanatory variables *x1*, or *x2*. (c) Strong correlation between the two explanatory variables *x1* and *x2*&#34; width=&#34;50%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Figure 1: (a and b) Little evidence of a relationship between the response variable *y* and either of the two explanatory variables *x1*, or *x2*. (c) Strong correlation between the two explanatory variables *x1* and *x2*&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;To reveal the relationship between &lt;em&gt;y&lt;/em&gt; and &lt;em&gt;x1&lt;/em&gt; we need to &lt;em&gt;control&lt;/em&gt; for the variation in &lt;em&gt;x2&lt;/em&gt;. One way to do this is to divide the data into subsets in each of which there is relatively little variation in &lt;em&gt;x2&lt;/em&gt;. With the following code we add a variable to the data set that contains categories of variation in &lt;em&gt;x2&lt;/em&gt;. I.e. we &lt;em&gt;cut&lt;/em&gt; the variation in &lt;em&gt;x2&lt;/em&gt; into 10 groups, and put the names of these groups in a new variable:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;dd &amp;lt;- mutate(dd,
            x2_cut = cut(x2, 10))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Figure 2, in particular the panels with more observations, clearly shows the positive relationships that we know exist. Great! Have a go at making an analogous plot while controlling for variation in &lt;em&gt;x1&lt;/em&gt;. (By the way, we have more data in the middle, because &lt;em&gt;x2&lt;/em&gt; is normally distriuted.)&lt;/p&gt;

&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;
&lt;img src=&#34;https://r4all.org/posts/2020-03-16-lurking-variables-and-hidden-relationships_files/figure-html/masked-rels-2-1.png&#34; alt=&#34;The positive relationship between the response variable (*y*) and one of the explanatory variables (*x1*) is visible because each facet shows a relatively small range of variation in the other explanatory variable (*x2*).&#34; width=&#34;50%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Figure 2: The positive relationship between the response variable (*y*) and one of the explanatory variables (*x1*) is visible because each facet shows a relatively small range of variation in the other explanatory variable (*x2*).&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;Another way to &lt;em&gt;control&lt;/em&gt; for variation in each explanatory variable while &amp;ldquo;viewing&amp;rdquo; the relationship of the response variable with the other is &lt;em&gt;multiple regression&lt;/em&gt;. Below we see very strong evidence of the positive relationship between &lt;em&gt;y&lt;/em&gt; and &lt;em&gt;x1&lt;/em&gt; and negative between &lt;em&gt;y&lt;/em&gt; and &lt;em&gt;x2&lt;/em&gt;, and that estimated coefficients (slopes) are not different from the real ones (1). Whereas the univariate regression show much weaker evidence of any relationships, and estimated coefficients are poorly estimated.&lt;/p&gt;

&lt;p&gt;Multiple regression is, in effect, doing what we did when we cut up the data and plotted parts of it that contained little variation in the other variable.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;mod_x1x2 &amp;lt;- lm(y ~ x1 + x2, data = dd)
summary(mod_x1x2)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = y ~ x1 + x2, data = dd)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -2.27190 -0.64723 -0.04082  0.68422  2.73434 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)  0.07772    0.09878   0.787    0.433    
## x1           1.13521    0.15776   7.196 1.31e-10 ***
## x2          -1.26254    0.16259  -7.765 8.44e-12 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.983 on 97 degrees of freedom
## Multiple R-squared:  0.3996,	Adjusted R-squared:  0.3872 
## F-statistic: 32.27 on 2 and 97 DF,  p-value: 1.801e-11
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;mod_x1 &amp;lt;- lm(y ~ x1, data = dd)
summary(mod_x1)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = y ~ x1, data = dd)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -3.02457 -0.72337  0.00238  0.79139  2.95465 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&amp;gt;|t|)
## (Intercept)  0.01596    0.12473   0.128    0.898
## x1           0.21467    0.13186   1.628    0.107
## 
## Residual standard error: 1.245 on 98 degrees of freedom
## Multiple R-squared:  0.02633,	Adjusted R-squared:  0.0164 
## F-statistic:  2.65 on 1 and 98 DF,  p-value: 0.1067
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;mod_x2 &amp;lt;- lm(y ~ x2, data = dd)
summary(mod_x2)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = y ~ x2, data = dd)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -2.79342 -0.95557 -0.02311  0.94306  2.39910 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&amp;gt;|t|)   
## (Intercept)  0.06106    0.12167   0.502  0.61689   
## x2          -0.38332    0.13218  -2.900  0.00461 **
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.211 on 98 degrees of freedom
## Multiple R-squared:  0.07904,	Adjusted R-squared:  0.06964 
## F-statistic:  8.41 on 1 and 98 DF,  p-value: 0.004606
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Maximum likelihood part 1</title>
      <link>https://r4all.org/posts/maximum-likelihood-part-1/</link>
      <pubDate>Wed, 27 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>https://r4all.org/posts/maximum-likelihood-part-1/</guid>
      <description>&lt;p&gt;This is the first in a series of posts about maximum likelihood methods for fitting statistical models to data. Inspiration for the material comes in large part from Drew Purves who presented something similar. Owen is using Drew’s approach as the basis for this course. Much of the R specific stuff is heavily influenced by Ben Bolker’s excellent book: Ecological Models and Data in R. The goal of this and the following posts includes:
learning how to fit to our data more mechanistic models of arbitrary complexity.
learning how to do this with ease, confidence, and complete transparency.
at some point delving into robust and efficient parameter estimation methods (e.g., MCMC).
at some point working out how to switch between frequentist and Bayesian approaches with ease.
(Please note that the focus of this post is learning about maximum likelihood methods. R is a only a tool to help that learning, so we avoid putting lots of potentially distracting R code in the post, and rather make it available as a separate file.)
Lets start on familiar ground by consider a common statistical approach, linear regression. This involves fitting a model (equation) to the observed data. The equation is:&lt;/p&gt;

&lt;p&gt;$$ y = a + bx + norm(0, \sigma) $$&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;$ y $ is the observed response variable&lt;/li&gt;
&lt;li&gt;$ x $ is the explanatory variable&lt;/li&gt;
&lt;li&gt;$ a $ is the intercept&lt;/li&gt;
&lt;li&gt;$ b $ is the slope&lt;/li&gt;
&lt;li&gt;$ norm(0, \sigma) $ is the error term.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This is the model we fit to our data. We are trying to find the value of $ a $, $ b $, and $ \sigma $ that fit the data. These are the parameters of the model. How do we find out the values of a and b that give the best regression line? Regression does so by minimizing the sum of squares, which is why we often use the term ‘least squares regression’. We are going to learn how to estimate the parameters of a model by maximising likelihood (instead of minimising least squares). We will start by considering a dataset and model even simpler than linear regression. This example may seem rather trivial. It is. However, better to start simple, with an example adequate to introduce many of the fundamental concepts we need.&lt;/p&gt;

&lt;p&gt;Assume we’ve counted the number of individuals in seven replicate quadrats that we’ve placed randomly in a field (don’t ask me why seven quadrats – maybe we left the other three in the lab – who knows). This is our observed data (seven numbers).&lt;/p&gt;

&lt;p&gt;Now, the definition of likelihood is $ p(data | model) $&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;$ p $ stands for probability&lt;/li&gt;
&lt;li&gt;* data * is the observed data&lt;/li&gt;
&lt;li&gt;$ | $ (the vertical bar) can be read as ‘given the’&lt;/li&gt;
&lt;li&gt;$ model $ = whatever model we like.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;That is, $ p(data | model) $ can be read as “the probability of the data given the model”.&lt;/p&gt;

&lt;p&gt;In our current example we will assume that individuals in the field are randomly distributed, and therefore that the count in each quadrat comes from a Poission distribution. The Poisson distribution has only one pa- rameter, the mean. This is what we want to estimate. Since this is a trivial example, we can easily find the most likely value of the mean by taking the arithmetic average (mean) of the observed values. We’re not go- ing to do this yet, since the purpose of using this example is illustration of concepts.&lt;/p&gt;

&lt;p&gt;So, our model is $ Y_i = Poisson(M) $, where $ Y_i $ is the ith (pronounced eye–eth) observation of the response variable (here the number of individuals counted in the first quadrat) and $ M $ is the mean of the Poisson distribution – this is what we want to estimate.&lt;/p&gt;

&lt;p&gt;We want to find the likelihood – $ p(data | model) $ – and so first we calculate the likelihood of each individual observation. The likelihood of the ith observation is,&lt;/p&gt;

&lt;p&gt;$$ p(Y_i | M) $$&lt;/p&gt;

&lt;p&gt;This is the probability of getting the value $ Y_i $ given the mean $ M $.
So if the first count $ Y_1 = 6 $ and we guess a mean of 7, we can find the probability of $ Y_1 = 6 $ in Excel with: &lt;code&gt;Poisson(6, 7, FALSE)&lt;/code&gt; (why false?). Or in R using &lt;code&gt;dpois(6, 7)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;If you do either of these you will find that the probability of observing a 6 given a Poisson distribution with mean 7 is about 0.15.&lt;/p&gt;

&lt;p&gt;We do this for each value of the response variable, log the probabilities, and then add them up.
Lets do this in Excel, using the worksheet ‘eg1’ in &lt;a href=&#34;https://r4all.org/files/eg1.xlsx&#34; target=&#34;_blank&#34;&gt;this Excel spreadsheet&lt;/a&gt;. Here is a screen grab of the worksheet:&lt;/p&gt;




  

&lt;figure&gt;

&lt;img src=&#34;https://r4all.org/img/excelgrab1.jpg&#34; /&gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  &lt;h4&gt;&lt;/h4&gt;
  
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The first column (from row 4 down) is the data, the second is the probability of each data point given the mean (look at the formula to see the call to the Excel Possion function). The third column is the log (base e; ln) of the probabilities (i.e., the log-likelihood. Often this is just called the likelihood, but we should try to always use the term log-likelihood, for accuracy. At the bottom of column C is the sum of the log-likelihoods. Cell A2 contains our guess of the mean of the observed data.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Look at the formulas in the cells to check they are what you expect.&lt;/li&gt;
&lt;li&gt;Try changing the guess of the mean and see what happens to the sum of the log probabilities.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;By changing the guess of the mean (cell A2 in the worksheet ‘eg1’) you change the log-likelihood. The challenge is to find the value of the mean that will minimise the log-likelihood (= maximise the likelihood). This value (estimate) of the mean is called the maximum likelihood estimate. We are interested in maximising the likelihood, or equivalently, minimising it (make as close to zero as possible). The more negative the sum of log likelihood the worse the model (guess of the mean) is. (Read this paragraph while playing with the Excel worksheet, until you are sure you get it. It is very important.)&lt;/p&gt;

&lt;p&gt;(By the way, we log the probabilities for a few reasons, including that computers often have problems deal- ing with very small numbers.)&lt;/p&gt;

&lt;p&gt;By playing with the guess of the mean in cell A2 of the Excel worksheet, you may have found that a value close to 6 minimises the log-likelihood (makes it closest to zero, i.e., least negative). You have just obtained a maximum likelihood estimate of the parameter of this model.&lt;/p&gt;

&lt;p&gt;Now take a look at the R script file &lt;a href=&#34;https://r4all.org/files/eg1.r&#34; target=&#34;_blank&#34;&gt;eg1.r&lt;/a&gt;. This shows how you can do with R what you just did with Excel.&lt;/p&gt;

&lt;p&gt;Of course, we don’t want to have to guess the mean, look at the log-likelihood, guess again, and look at the log-likelihood again, and so on. We want the computer to do the work. In the R file is some script that makes a vector of guesses of the mean, loops through these, and records the log-likelihood. Then we can plot the value of the log-likelihood versus the guess of the mean.&lt;/p&gt;




  

&lt;figure&gt;

&lt;img src=&#34;https://r4all.org/img/like_profile.jpg&#34; /&gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  &lt;h4&gt;&lt;/h4&gt;
  
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The maximum of this curve is the maximum likelihood. It is achieved when the guess of the mean has the val- ue of 6.1, where the vertical dashed line is drawn. The log-likelihood here is -14.821. This curve is also know as the likelihood profile.&lt;/p&gt;

&lt;p&gt;How did we do? Our guess was 6.1 and the actual mean is 6.143, with a log-likelihood of -14.820. Not bad then!
Question for you: What should we change in the R script to get a more accurate estimate of the mean?
Now, this is important. If we suspected our observed data were poisson distributed, and we wanted to mod- el that data in R, we might easily decide to use the generalised linear model funtion &lt;code&gt;glm&lt;/code&gt; and specify a pois- son distribution (family=poisson). The code to do this would look something like, &lt;code&gt;glm(n ~ 1, family=poisson)&lt;/code&gt; where n is the response variable that here is the number of individuals in each of the quadrats.&lt;/p&gt;

&lt;p&gt;This model is in the R script previously mentioned, and the coefficient (estimated parameter / mean) is 1.8153. Not the mean that we found (6.1). This is because specifying a poisson distribution (family=poisson) means that a log link function is used. So we have to un-log the coefficient to get the actual value&amp;hellip; exp(1.8153) = 6.1428. Good.&lt;/p&gt;

&lt;p&gt;What else can we do with this simple example? Let’s figure out for ourselves the AIC (Akaike Information Cri- teria) of our model. The definition of AIC is,&lt;/p&gt;

&lt;p&gt;$$ AIC = -2*L + 2*n $$&lt;/p&gt;

&lt;p&gt;Where L is the log-likelihood and p is the number of parameters (only one, the mean, in our model). Our log-likelihood was -14.82117, so AIC = 31.64. The AIC given by the &lt;code&gt;glm&lt;/code&gt; function is 31.64 also. Great.&lt;/p&gt;

&lt;p&gt;How does &lt;code&gt;glm&lt;/code&gt; work? How does it find the mean? It tries many guesses of the mean and sees which gives the maximum likelihood, just like we did in the R script that made the graph above. However, &lt;code&gt;glm&lt;/code&gt; uses a search method that is a bit smarter than ours (I think it calls the function &lt;code&gt;optim&lt;/code&gt; which by default uses the Nelder-Mead search algorith, but this is not too important now). What is important is to realise that &lt;code&gt;glm&lt;/code&gt; is just searching around the parameter space looking for a maximum likelihood. It tries to go uphill on the likelihood profile. When it can’t go uphill any more, it knows it’s reached the maximum likelihood.
You should now be able to have a good go at answering these questions:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;What are parameters?&lt;/li&gt;
&lt;li&gt;What is a variable?&lt;/li&gt;
&lt;li&gt;What is likelihood?&lt;/li&gt;
&lt;li&gt;What does this mean p(data| model)?&lt;/li&gt;
&lt;li&gt;How can we maximise likelihood.&lt;/li&gt;
&lt;li&gt;What is the maximum likelihood estimate of a parameter?&lt;/li&gt;
&lt;li&gt;What is a likelihood profile?&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Why adjust your r-squared</title>
      <link>https://r4all.org/posts/why-adjust-your-r-squared/</link>
      <pubDate>Wed, 27 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>https://r4all.org/posts/why-adjust-your-r-squared/</guid>
      <description>&lt;p&gt;Just a little demo of what happens if you don’t or do adjust your r-squared. Here’s the bottom line&amp;hellip;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://r4all.org/posts/2019-02-27-why-adjust-your-r-squared_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;

&lt;p&gt;As we increase the number of explanatory variables in a linear model (e.g. multiple regression) the unadjusted r-squared increaes (green dots) even if the additional explanatory variables contain only random numbers. The adjusted r-squared is &amp;ldquo;adjusted&amp;rdquo; so it does not! So if we simply want to know the proportion of variance explained by our model we are fine using the unadjusted r-squared. If, however, we want to compare the r-squared of models with different numbers of explanatory variables, we should compare the adjusted r-squared.&lt;/p&gt;

&lt;p&gt;Here&amp;rsquo;s the code for making the figure. (Done before we converted to the tidyverse!)&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## Lets do some multiple regression, with different numbers of explanatory variables
## with completely random data
numb.expl.vars &amp;lt;- floor(rep(2^seq(0, 5, 0.5), each=50))

## Number of observations
n &amp;lt;- 100

## The response variable
y &amp;lt;- rnorm(n)

## Function to return the unadjusted and adjusted r-squared
get.r2 &amp;lt;- function(ne) {
  x &amp;lt;- as.data.frame(matrix(rnorm(n*ne), n, ne))
  m1 &amp;lt;- lm(y ~ ., x)
  result &amp;lt;- c(summary(m1)$r.squared, summary(m1)$adj.r.squared)
  result
}

## use lapply to run the function over the number of explanatory variables vec
rez &amp;lt;- do.call(rbind, lapply(numb.expl.vars, function(x) get.r2(x)))

## get the mean r-squared and adjusted r-squared per number of expl varbs
means &amp;lt;- aggregate(rez, list(numb.expl.vars=numb.expl.vars), mean)

## plot the data
matplot(log2(numb.expl.vars), rez, type=&amp;quot;n&amp;quot;, ann=F, axes=F)
box()
abline(h=0)
matpoints(jitter(log2(numb.expl.vars)), rez, pch=19, col=c(&amp;quot;#11ff1144&amp;quot;, &amp;quot;#ff111144&amp;quot;))
mtext(1, line=2.5, text=&amp;quot;Number of explanatory variables&amp;quot;)
mtext(2, line=2, text=&amp;quot;R-squared\n(green unadjusted, red adjusted)&amp;quot;)
axis(1, at=0:5, labels=2^(0:5))
axis(2)
matpoints(log2(means[,1]), means[,2], pch=21, bg=c(&amp;quot;#11ff1144&amp;quot;))
matpoints(log2(means[,1]), means[,3], pch=21, bg=c(&amp;quot;#ff111144&amp;quot;))

## for fun, calculate the adjusted r-squared manually
adj.rsquared &amp;lt;- 1 - (1-rez[,1])*(n-1)/(n-numb.expl.vars-1)
sum(abs(adj.rsquared-rez[,2])&amp;gt;1e-10) ## should be zero
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Getting lost packages</title>
      <link>https://r4all.org/posts/getting-lost-packages/</link>
      <pubDate>Tue, 05 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>https://r4all.org/posts/getting-lost-packages/</guid>
      <description>&lt;p&gt;To use the functions in an add-on package you first need to install the package. Remember you only need install it once.&lt;/p&gt;

&lt;p&gt;During the writing of the book, and in early 2018 the normal method for installing the &lt;strong&gt;ggfortify&lt;/strong&gt; add-on package didn’t work (we got the message &lt;code&gt;package ggfortify is not available (for R Version 3.2.4)&lt;/code&gt;).&lt;/p&gt;

&lt;p&gt;This has not happened for some time, so hopefully you won&amp;rsquo;t experience it. If you do&amp;hellip;&lt;/p&gt;

&lt;p&gt;Many packages are now developed in an environment / web service called &lt;em&gt;GitHub&lt;/em&gt;, and R has an interface for installing packages directly from &lt;em&gt;GitHub&lt;/em&gt;. This is true for &lt;em&gt;ggfortify&lt;/em&gt;. Here is how you can access the package that way:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;install.packages(&amp;quot;devtools&amp;quot;)
library(devtools)
install_github(&#39;sinhrks/ggfortify&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Another option: if the packages was once on CRAN, and archived, older version may still be available. Probably you can find this by googling ggfortify cran and following links to a rather unfriendly looking web page that lists a few files, one of which was, at the time of writing this, gfortify_0.1.0.tar.gz. Download this to your computer, making sure you know which folder it gets downloaded into. Then, in Rstudio, Click the Install button in the Packages (as usual when you want to install a package). However, instead of Installing from: Repository CRAN choose Install from: Package Archive File (.tgz, .tar.gz). Then you need to click Browse and find the file you just downloaded. Then click Install. Hopefully it will still work with the version of R you have installed.&lt;/p&gt;

&lt;p&gt;Another option is to do it the old fashioned way. E.g. instead of using the autoplot() function from the ggfortify package, do this:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;par(mfrow(2,2))
plot(model, add.smooth = FALSE)
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Importing data update</title>
      <link>https://r4all.org/posts/importing-data-update/</link>
      <pubDate>Tue, 05 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>https://r4all.org/posts/importing-data-update/</guid>
      <description>&lt;p&gt;As the Second Edition of &lt;em&gt;Getting Started with R&lt;/em&gt; was going to press, Rstudio changed the function it uses to import data in the &lt;strong&gt;Import Dataset&lt;/strong&gt; tool, from the base function &lt;code&gt;read.csv()&lt;/code&gt; to the &lt;code&gt;read_csv()&lt;/code&gt; function in the &lt;strong&gt;readr&lt;/strong&gt; package. Since then, the &lt;strong&gt;Import Dataset&lt;/strong&gt; button gives a menu with an option to use either (&amp;ldquo;base&amp;rdquo; uses &lt;code&gt;read.csv&lt;/code&gt; and &amp;ldquo;readr&amp;rdquo; uses &lt;code&gt;read_csv&lt;/code&gt;) From the Rstudio Blog about the &lt;strong&gt;readr&lt;/strong&gt; package:&lt;/p&gt;

&lt;p&gt;Compared to base equivalents like &lt;code&gt;read.csv&lt;/code&gt;, readr is much faster and gives more convenient output: it never converts strings to factors, can parse date/times, and it doesn’t munge the column names.&lt;/p&gt;

&lt;p&gt;Great! If you have dates and times in a column, its possible that &lt;code&gt;read_csv&lt;/code&gt; will see this and then correctly format them as dates. So you wouldn’t need to do this yourself (covered in Appendix of Chapter 2 of the Second Edition).&lt;/p&gt;

&lt;p&gt;Something else you might notice is that the data appears different if you look at it in the Console. This is because &lt;code&gt;read_csv&lt;/code&gt; brings the data in as a special type of object called a &lt;em&gt;tibble&lt;/em&gt;. (The standard &lt;code&gt;read.csv&lt;/code&gt; function bring the data in as a standard data.frame.) You can read all about tibbles on the RStudio blog. Why are they better than data.frames? First, they only show the first ten lines and as many variables as will comfortably fit when you look at them in the Console, and the type of variable is given, and the number of additional rows and variables not displayed. E.g.:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(readr)
dd &amp;lt;- read_csv(readr_example(&amp;quot;mtcars.csv&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## Parsed with column specification:
## cols(
##   mpg = col_double(),
##   cyl = col_double(),
##   disp = col_double(),
##   hp = col_double(),
##   drat = col_double(),
##   wt = col_double(),
##   qsec = col_double(),
##   vs = col_double(),
##   am = col_double(),
##   gear = col_double(),
##   carb = col_double()
## )
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;dd
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## # A tibble: 32 x 11
##      mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb
##    &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
##  1  21       6  160    110  3.9   2.62  16.5     0     1     4     4
##  2  21       6  160    110  3.9   2.88  17.0     0     1     4     4
##  3  22.8     4  108     93  3.85  2.32  18.6     1     1     4     1
##  4  21.4     6  258    110  3.08  3.22  19.4     1     0     3     1
##  5  18.7     8  360    175  3.15  3.44  17.0     0     0     3     2
##  6  18.1     6  225    105  2.76  3.46  20.2     1     0     3     1
##  7  14.3     8  360    245  3.21  3.57  15.8     0     0     3     4
##  8  24.4     4  147.    62  3.69  3.19  20       1     0     4     2
##  9  22.8     4  141.    95  3.92  3.15  22.9     1     0     4     2
## 10  19.2     6  168.   123  3.92  3.44  18.3     1     0     4     4
## # … with 22 more rows
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In summary, everything in the Second Edition will work just fine, but what you see in the Console might look a little different at times.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Datasets</title>
      <link>https://r4all.org/books/datasets/</link>
      <pubDate>Sun, 09 Sep 2018 00:00:00 +0200</pubDate>
      
      <guid>https://r4all.org/books/datasets/</guid>
      <description>

&lt;h2 id=&#34;datasets-used-in-getting-started-with-r-second-edition&#34;&gt;Datasets used in &lt;em&gt;Getting Started with R&lt;/em&gt;, Second Edition&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/R4All/datasets/archive/master.zip&#34;&gt;All the datasets (and more) one zip file&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Datasets for the first edition of &lt;em&gt;Getting Started with R&lt;/em&gt; are also contained in that zip file.&lt;/p&gt;

&lt;h2 id=&#34;datasets-used-in-insights&#34;&gt;Datasets used in &lt;em&gt;Insights&lt;/em&gt;&lt;/h2&gt;

&lt;p&gt;The datasets used in &lt;em&gt;Insights&lt;/em&gt; are all available directly from publically accessible data repositories. Please see the book or &lt;a href=&#34;http://insightsfromdata.io/&#34;&gt;companion website&lt;/a&gt; for details.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Getting Started with R, Second Edition</title>
      <link>https://r4all.org/books/gswr2/</link>
      <pubDate>Sun, 09 Sep 2018 00:00:00 +0200</pubDate>
      
      <guid>https://r4all.org/books/gswr2/</guid>
      <description>




  

&lt;figure&gt;

&lt;img src=&#34;https://r4all.org/img/SecondEdCover.png&#34; /&gt;


&lt;/figure&gt;

&lt;p&gt;We are pleased to announce the second edition of Getting Started with R
Paperback at &lt;a href=&#34;https://global.oup.com/academic/product/getting-started-with-r-9780198787846?cc=ch&amp;amp;lang=en&amp;amp;&#34;&gt;OUP&lt;/a&gt;, &lt;a href=&#34;https://www.amazon.co.uk/Getting-Started-R-Introduction-Biologists/dp/0198787847&#34;&gt;Amazon UK&lt;/a&gt;, &lt;a href=&#34;https://www.amazon.com/Getting-Started-R-Introduction-Biologists/dp/0198787847&#34;&gt;Amazon.com&lt;/a&gt;, &lt;a href=&#34;https://www.amazon.de/Getting-Started-R-Andrew-Beckerman/dp/0198787847&#34;&gt;Amazon.de&lt;/a&gt;, &lt;a href=&#34;https://www.orellfuessli.ch/shop/home/suchartikel/ID46438190.html&#34;&gt;Buch.ch&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://www.oxfordscholarship.com/view/10.1093/acprof:oso/9780198787839.001.0001/acprof-9780198787839&#34;&gt;Electric version at Oxford Scholarship Online.&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;We got great feedback about our book &amp;ldquo;Getting Started with R, An Introduction for Biologists&amp;rdquo;, but realised over the last couple of years that we were using and teaching R very differently from what we wrote about back in 2010. So we have almost completely rewritten for a very different and new Second Edition.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Now based on using RStudio&lt;/li&gt;
&lt;li&gt;Now based around dplyr and ggplot2 for data management and grahics&lt;/li&gt;
&lt;li&gt;New basic examples for regression, 1-way ANOVA and 2-way ANOVA&lt;/li&gt;
&lt;li&gt;A whole new chapter on the Generalised Linear Model&lt;/li&gt;
&lt;li&gt;That same Getting Started with R attitude.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;While many of the tools and functions have changed in this new edition, the content remains clear and enjoyable. We still walk your through, in that R4All style, from the basics of setting up your computer, through to data import and exploration and ultimately to a series of statistical models for interpreting data and experiments.&lt;/p&gt;

&lt;h2 id=&#34;what-is-so-different-and-new&#34;&gt;What is so different and new?&lt;/h2&gt;

&lt;p&gt;Several recent developments in the R community have made it even easier for us to help you focus more on your data and questions. RStudio has emerged as a brilliant cross-platform interface to working with R. Furthermore, Hadley Wickham and colleagues, have developed several add-on packages including as dplyr, tidyr and ggplot2 that not only provide consistent and intuitive ways to work with your data, but are emerging as industry standards (academic and non-academic).&lt;/p&gt;

&lt;p&gt;We couldn&amp;rsquo;t afford to ignore these developments. The second edition thus dispenses entirely with classic methods of using R, and instead embraces RStudio along with dplyr, tidyr and ggplot2.&lt;/p&gt;

&lt;p&gt;You&amp;rsquo;ll love it.&lt;/p&gt;

&lt;h2 id=&#34;anything-else-new&#34;&gt;Anything else new?&lt;/h2&gt;

&lt;p&gt;We have a new co-author, Dylan Childs. He better at using R than Owen and Andrew, and kind of grumpy, so fits in well.  He&amp;rsquo;s also helped us expand our stats offerings in the book.  Not only do we provide more examples for basic stats (Regression, 1-way ANOVA, 2-way ANOVA) but a whole new chapter on the Generalised Linear Model.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
