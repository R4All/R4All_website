[{"authors":null,"categories":null,"content":"Owen has also used R for nearly 20 years, and has particular expertise in teaching beginners, multivariate statistics, spatial data, programming, maximum likelihood estimation, and visualisation (i.e., nice graphs!). His research focuses on the causes and consequences of extinctions in a changing world. His group performs experiments with microbial communities, models the structure of food webs, analyses variation in biodiversity, and does fieldwork in Iceland, the UK, and Switzerland.\n","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"598b63dd58b43bce02403646f240cd3c","permalink":"/author/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/admin/","section":"author","summary":"Owen has also used R for nearly 20 years, and has particular expertise in teaching beginners, multivariate statistics, spatial data, programming, maximum likelihood estimation, and visualisation (i.e., nice graphs!). His research focuses on the causes and consequences of extinctions in a changing world. His group performs experiments with microbial communities, models the structure of food webs, analyses variation in biodiversity, and does fieldwork in Iceland, the UK, and Switzerland.","tags":null,"title":"","type":"author"},{"authors":null,"categories":null,"content":"Andrew is an evolutionary ecologists. He is interested in how the environment affects organism traits and how this determines the distribution and abundance of species and the complexity and structure of their communities. He\u0026rsquo;s been using R for more than 20 years and specialises in enthusiastically explaining things three or four different ways. He still likes learning to use new methods and tools. He also rides bicycles a fair bit.\n","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"c22df01f6f5d728d9ffcb11323d417b6","permalink":"/author/andrew/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/andrew/","section":"author","summary":"Andrew is an evolutionary ecologists. He is interested in how the environment affects organism traits and how this determines the distribution and abundance of species and the complexity and structure of their communities. He\u0026rsquo;s been using R for more than 20 years and specialises in enthusiastically explaining things three or four different ways. He still likes learning to use new methods and tools. He also rides bicycles a fair bit.","tags":null,"title":"","type":"author"},{"authors":null,"categories":null,"content":"Dylan has also used R for nearly 20 years, and has particular expertise in teaching beginners, additive models, numerical simulation techniques, integral projection models, and data visualisation. His research focuses on the environmental and demographic drivers of population dynamics and selection. His group models all kinds of organisms, from arable weeds such as black grass to the Soay sheep of St Kilda.\n","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"b67a896c1bc60973e821a3db923e6ab2","permalink":"/author/dylan/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/dylan/","section":"author","summary":"Dylan has also used R for nearly 20 years, and has particular expertise in teaching beginners, additive models, numerical simulation techniques, integral projection models, and data visualisation. His research focuses on the environmental and demographic drivers of population dynamics and selection. His group models all kinds of organisms, from arable weeds such as black grass to the Soay sheep of St Kilda.","tags":null,"title":"","type":"author"},{"authors":null,"categories":null,"content":"Natalie Cooper\u0026rsquo;s reasearch aims to understand broad-scale patterns of biodiversity. She uses cutting-edge phylogenetic comparative methods and various large datasets to investigate a variety of topics using R. She currently has projects on whales, snakes, dinosaurs, bryozoans, corals, pangolins, tenrecs, fishes, chameleons, amphibians and many more! Natalie is an advocate for diversity in STEM and reproducibility. In her spare time she likes to read, watch Netflix and climb mountains, while sticking it to the patriarchy.\n","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"3c08f1613c62381704970378bad25944","permalink":"/author/natalie/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/natalie/","section":"author","summary":"Natalie Cooper\u0026rsquo;s reasearch aims to understand broad-scale patterns of biodiversity. She uses cutting-edge phylogenetic comparative methods and various large datasets to investigate a variety of topics using R. She currently has projects on whales, snakes, dinosaurs, bryozoans, corals, pangolins, tenrecs, fishes, chameleons, amphibians and many more! Natalie is an advocate for diversity in STEM and reproducibility. In her spare time she likes to read, watch Netflix and climb mountains, while sticking it to the patriarchy.","tags":null,"title":"","type":"author"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"d41d8cd98f00b204e9800998ecf8427e","permalink":"/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"author","summary":"","tags":null,"title":"Authors","type":"author"},{"authors":null,"categories":null,"content":"Please look in the menu on the left (or elsewhere) for information about the book Getting Started with R, the datasets used in the book, and some miscellany.\n","date":1536444000,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536444000,"objectID":"b905e45f7194913eeeed179620391d24","permalink":"/books/","publishdate":"2018-09-09T00:00:00+02:00","relpermalink":"/books/","section":"books","summary":"Please look in the menu on the left (or elsewhere) for information about the book Getting Started with R, the datasets used in the book, and some miscellany.","tags":null,"title":"Overview","type":"docs"},{"authors":null,"categories":null,"content":"","date":1461103200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461103200,"objectID":"3769b896f8c2f656f609ef66d8bfd1f5","permalink":"/who-are-we/owen/","publishdate":"2016-04-20T00:00:00+02:00","relpermalink":"/who-are-we/owen/","section":"who-are-we","summary":"","tags":null,"title":" ","type":"who-are-we"},{"authors":null,"categories":null,"content":"","date":1461103200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461103200,"objectID":"0bfeec153670ac4794874b16bf7f9d76","permalink":"/who-are-we/natalie/","publishdate":"2016-04-20T00:00:00+02:00","relpermalink":"/who-are-we/natalie/","section":"who-are-we","summary":"","tags":null,"title":" ","type":"who-are-we"},{"authors":null,"categories":null,"content":"","date":1461103200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461103200,"objectID":"102bb2725d25d84db6b1d3f0146dd443","permalink":"/who-are-we/andrew/","publishdate":"2016-04-20T00:00:00+02:00","relpermalink":"/who-are-we/andrew/","section":"who-are-we","summary":"","tags":null,"title":" ","type":"who-are-we"},{"authors":null,"categories":null,"content":"","date":1461103200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461103200,"objectID":"7f683e292b0f5fa1e163249c15004108","permalink":"/who-are-we/dylan/","publishdate":"2016-04-20T00:00:00+02:00","relpermalink":"/who-are-we/dylan/","section":"who-are-we","summary":"","tags":null,"title":" ","type":"who-are-we"},{"authors":null,"categories":[],"content":"This is the first in a series of posts about maximum likelihood methods for fitting statistical models to data. Inspiration for the material comes in large part from Drew Purves who presented something similar. Owen is using Drew’s approach as the basis for this course. Much of the R specific stuff is heavily influenced by Ben Bolker’s excellent book: Ecological Models and Data in R. The goal of this and the following posts includes: learning how to fit to our data more mechanistic models of arbitrary complexity. learning how to do this with ease, confidence, and complete transparency. at some point delving into robust and efficient parameter estimation methods (e.g., MCMC). at some point working out how to switch between frequentist and Bayesian approaches with ease. (Please note that the focus of this post is learning about maximum likelihood methods. R is a only a tool to help that learning, so we avoid putting lots of potentially distracting R code in the post, and rather make it available as a separate file.) Lets start on familiar ground by consider a common statistical approach, linear regression. This involves fitting a model (equation) to the observed data. The equation is:\n$$ y = a + bx + norm(0, \\sigma) $$\n $ y $ is the observed response variable $ x $ is the explanatory variable $ a $ is the intercept $ b $ is the slope $ norm(0, \\sigma) $ is the error term.  This is the model we fit to our data. We are trying to find the value of $ a $, $ b $, and $ \\sigma $ that fit the data. These are the parameters of the model. How do we find out the values of a and b that give the best regression line? Regression does so by minimizing the sum of squares, which is why we often use the term ‘least squares regression’. We are going to learn how to estimate the parameters of a model by maximising likelihood (instead of minimising least squares). We will start by considering a dataset and model even simpler than linear regression. This example may seem rather trivial. It is. However, better to start simple, with an example adequate to introduce many of the fundamental concepts we need.\nAssume we’ve counted the number of individuals in seven replicate quadrats that we’ve placed randomly in a field (don’t ask me why seven quadrats – maybe we left the other three in the lab – who knows). This is our observed data (seven numbers).\nNow, the definition of likelihood is $ p(data | model) $\n $ p $ stands for probability * data * is the observed data $ | $ (the vertical bar) can be read as ‘given the’ $ model $ = whatever model we like.  That is, $ p(data | model) $ can be read as “the probability of the data given the model”.\nIn our current example we will assume that individuals in the field are randomly distributed, and therefore that the count in each quadrat comes from a Poission distribution. The Poisson distribution has only one pa- rameter, the mean. This is what we want to estimate. Since this is a trivial example, we can easily find the most likely value of the mean by taking the arithmetic average (mean) of the observed values. We’re not go- ing to do this yet, since the purpose of using this example is illustration of concepts.\nSo, our model is $ Y_i = Poisson(M) $, where $ Y_i $ is the ith (pronounced eye–eth) observation of the response variable (here the number of individuals counted in the first quadrat) and $ M $ is the mean of the Poisson distribution – this is what we want to estimate.\nWe want to find the likelihood – $ p(data | model) $ – and so first we calculate the likelihood of each individual observation. The likelihood of the ith observation is,\n$$ p(Y_i | M) $$\nThis is the probability of getting the value $ Y_i $ given the mean $ M $. So if the first count $ Y_1 = 6 $ and we guess a mean of 7, we can find the probability of $ Y_1 = 6 $ in Excel with: Poisson(6, 7, FALSE) (why false?). Or in R using dpois(6, 7)\nIf you do either of these you will find that the probability of observing a 6 given a Poisson distribution with mean 7 is about 0.15.\nWe do this for each value of the response variable, log the probabilities, and then add them up. Lets do this in Excel, using the worksheet ‘eg1’ in this Excel spreadsheet. Here is a screen grab of the worksheet:\n    The first column (from row 4 down) is the data, the second is the probability of each data point given the mean (look at the formula to see the call to the Excel Possion function). The third column is the log (base e; ln) of the probabilities (i.e., the log-likelihood. Often this is just called the likelihood, but we should try to always use the term log-likelihood, for accuracy. At the bottom of column C is the sum of the log-likelihoods. Cell A2 contains our guess of the mean of the observed data.\n Look at the formulas in the cells to check they are what you expect. Try changing the guess of the mean and see what happens to the sum of the log probabilities.  By changing the guess of the mean (cell A2 in the worksheet ‘eg1’) you change the log-likelihood. The challenge is to find the value of the mean that will minimise the log-likelihood (= maximise the likelihood). This value (estimate) of the mean is called the maximum likelihood estimate. We are interested in maximising the likelihood, or equivalently, minimising it (make as close to zero as possible). The more negative the sum of log likelihood the worse the model (guess of the mean) is. (Read this paragraph while playing with the Excel worksheet, until you are sure you get it. It is very important.)\n(By the way, we log the probabilities for a few reasons, including that computers often have problems deal- ing with very small numbers.)\nBy playing with the guess of the mean in cell A2 of the Excel worksheet, you may have found that a value close to 6 minimises the log-likelihood (makes it closest to zero, i.e., least negative). You have just obtained a maximum likelihood estimate of the parameter of this model.\nNow take a look at the R script file eg1.r. This shows how you can do with R what you just did with Excel.\nOf course, we don’t want to have to guess the mean, look at the log-likelihood, guess again, and look at the log-likelihood again, and so on. We want the computer to do the work. In the R file is some script that makes a vector of guesses of the mean, loops through these, and records the log-likelihood. Then we can plot the value of the log-likelihood versus the guess of the mean.\n    The maximum of this curve is the maximum likelihood. It is achieved when the guess of the mean has the val- ue of 6.1, where the vertical dashed line is drawn. The log-likelihood here is -14.821. This curve is also know as the likelihood profile.\nHow did we do? Our guess was 6.1 and the actual mean is 6.143, with a log-likelihood of -14.820. Not bad then! Question for you: What should we change in the R script to get a more accurate estimate of the mean? Now, this is important. If we suspected our observed data were poisson distributed, and we wanted to mod- el that data in R, we might easily decide to use the generalised linear model funtion glm and specify a pois- son distribution (family=poisson). The code to do this would look something like, glm(n ~ 1, family=poisson) where n is the response variable that here is the number of individuals in each of the quadrats.\nThis model is in the R script previously mentioned, and the coefficient (estimated parameter / mean) is 1.8153. Not the mean that we found (6.1). This is because specifying a poisson distribution (family=poisson) means that a log link function is used. So we have to un-log the coefficient to get the actual value\u0026hellip; exp(1.8153) = 6.1428. Good.\nWhat else can we do with this simple example? Let’s figure out for ourselves the AIC (Akaike Information Cri- teria) of our model. The definition of AIC is,\n$$ AIC = -2*L + 2*n $$\nWhere L is the log-likelihood and p is the number of parameters (only one, the mean, in our model). Our log-likelihood was -14.82117, so AIC = 31.64. The AIC given by the glm function is 31.64 also. Great.\nHow does glm work? How does it find the mean? It tries many guesses of the mean and sees which gives the maximum likelihood, just like we did in the R script that made the graph above. However, glm uses a search method that is a bit smarter than ours (I think it calls the function optim which by default uses the Nelder-Mead search algorith, but this is not too important now). What is important is to realise that glm is just searching around the parameter space looking for a maximum likelihood. It tries to go uphill on the likelihood profile. When it can’t go uphill any more, it knows it’s reached the maximum likelihood. You should now be able to have a good go at answering these questions:\n What are parameters? What is a variable? What is likelihood? What does this mean p(data| model)? How can we maximise likelihood. What is the maximum likelihood estimate of a parameter? What is a likelihood profile?  ","date":1551225600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1551225600,"objectID":"8a4f23d9bbf83a64e17351267ae46fd1","permalink":"/posts/maximum-likelihood-part-1/","publishdate":"2019-02-27T00:00:00Z","relpermalink":"/posts/maximum-likelihood-part-1/","section":"posts","summary":"This is the first in a series of posts about maximum likelihood methods for fitting statistical models to data. Inspiration for the material comes in large part from Drew Purves who presented something similar. Owen is using Drew’s approach as the basis for this course. Much of the R specific stuff is heavily influenced by Ben Bolker’s excellent book: Ecological Models and Data in R. The goal of this and the following posts includes: learning how to fit to our data more mechanistic models of arbitrary complexity.","tags":[],"title":"Maximum likelihood part 1","type":"posts"},{"authors":null,"categories":[],"content":"Just a little demo of what happens if you don’t or do adjust your r-squared. Here’s the bottom line\u0026hellip;\nAs we increase the number of explanatory variables in a linear model (e.g. multiple regression) the unadjusted r-squared increaes (green dots) even if the additional explanatory variables contain only random numbers. The adjusted r-squared is \u0026ldquo;adjusted\u0026rdquo; so it does not! So if we simply want to know the proportion of variance explained by our model we are fine using the unadjusted r-squared. If, however, we want to compare the r-squared of models with different numbers of explanatory variables, we should compare the adjusted r-squared.\nHere\u0026rsquo;s the code for making the figure. (Done before we converted to the tidyverse!)\n## Lets do some multiple regression, with different numbers of explanatory variables ## with completely random data numb.expl.vars \u0026lt;- floor(rep(2^seq(0, 5, 0.5), each=50)) ## Number of observations n \u0026lt;- 100 ## The response variable y \u0026lt;- rnorm(n) ## Function to return the unadjusted and adjusted r-squared get.r2 \u0026lt;- function(ne) { x \u0026lt;- as.data.frame(matrix(rnorm(n*ne), n, ne)) m1 \u0026lt;- lm(y ~ ., x) result \u0026lt;- c(summary(m1)$r.squared, summary(m1)$adj.r.squared) result } ## use lapply to run the function over the number of explanatory variables vec rez \u0026lt;- do.call(rbind, lapply(numb.expl.vars, function(x) get.r2(x))) ## get the mean r-squared and adjusted r-squared per number of expl varbs means \u0026lt;- aggregate(rez, list(numb.expl.vars=numb.expl.vars), mean) ## plot the data matplot(log2(numb.expl.vars), rez, type=\u0026quot;n\u0026quot;, ann=F, axes=F) box() abline(h=0) matpoints(jitter(log2(numb.expl.vars)), rez, pch=19, col=c(\u0026quot;#11ff1144\u0026quot;, \u0026quot;#ff111144\u0026quot;)) mtext(1, line=2.5, text=\u0026quot;Number of explanatory variables\u0026quot;) mtext(2, line=2, text=\u0026quot;R-squared\\n(green unadjusted, red adjusted)\u0026quot;) axis(1, at=0:5, labels=2^(0:5)) axis(2) matpoints(log2(means[,1]), means[,2], pch=21, bg=c(\u0026quot;#11ff1144\u0026quot;)) matpoints(log2(means[,1]), means[,3], pch=21, bg=c(\u0026quot;#ff111144\u0026quot;)) ## for fun, calculate the adjusted r-squared manually adj.rsquared \u0026lt;- 1 - (1-rez[,1])*(n-1)/(n-numb.expl.vars-1) sum(abs(adj.rsquared-rez[,2])\u0026gt;1e-10) ## should be zero  ","date":1551225600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1551225600,"objectID":"def77688abf7a41612350e17e9206f46","permalink":"/posts/why-adjust-your-r-squared/","publishdate":"2019-02-27T00:00:00Z","relpermalink":"/posts/why-adjust-your-r-squared/","section":"posts","summary":"Just a little demo of what happens if you don’t or do adjust your r-squared. Here’s the bottom line\u0026hellip;\nAs we increase the number of explanatory variables in a linear model (e.g. multiple regression) the unadjusted r-squared increaes (green dots) even if the additional explanatory variables contain only random numbers. The adjusted r-squared is \u0026ldquo;adjusted\u0026rdquo; so it does not! So if we simply want to know the proportion of variance explained by our model we are fine using the unadjusted r-squared.","tags":[],"title":"Why adjust your r-squared","type":"posts"},{"authors":null,"categories":[],"content":"To use the functions in an add-on package you first need to install the package. Remember you only need install it once.\nDuring the writing of the book, and in early 2018 the normal method for installing the ggfortify add-on package didn’t work (we got the message package ggfortify is not available (for R Version 3.2.4)).\nThis has not happened for some time, so hopefully you won\u0026rsquo;t experience it. If you do\u0026hellip;\nMany packages are now developed in an environment / web service called GitHub, and R has an interface for installing packages directly from GitHub. This is true for ggfortify. Here is how you can access the package that way:\ninstall.packages(\u0026quot;devtools\u0026quot;) library(devtools) install_github('sinhrks/ggfortify')  Another option: if the packages was once on CRAN, and archived, older version may still be available. Probably you can find this by googling ggfortify cran and following links to a rather unfriendly looking web page that lists a few files, one of which was, at the time of writing this, gfortify_0.1.0.tar.gz. Download this to your computer, making sure you know which folder it gets downloaded into. Then, in Rstudio, Click the Install button in the Packages (as usual when you want to install a package). However, instead of Installing from: Repository CRAN choose Install from: Package Archive File (.tgz, .tar.gz). Then you need to click Browse and find the file you just downloaded. Then click Install. Hopefully it will still work with the version of R you have installed.\nAnother option is to do it the old fashioned way. E.g. instead of using the autoplot() function from the ggfortify package, do this:\npar(mfrow(2,2)) plot(model, add.smooth = FALSE)  ","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"e953d378ca03f910a4ca7f88b0cfa570","permalink":"/posts/getting-lost-packages/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/posts/getting-lost-packages/","section":"posts","summary":"To use the functions in an add-on package you first need to install the package. Remember you only need install it once.\nDuring the writing of the book, and in early 2018 the normal method for installing the ggfortify add-on package didn’t work (we got the message package ggfortify is not available (for R Version 3.2.4)).\nThis has not happened for some time, so hopefully you won\u0026rsquo;t experience it. If you do\u0026hellip;","tags":[],"title":"Getting lost packages","type":"posts"},{"authors":null,"categories":[],"content":"As the Second Edition of Getting Started with R was going to press, Rstudio changed the function it uses to import data in the Import Dataset tool, from the base function read.csv() to the read_csv() function in the readr package. Since then, the Import Dataset button gives a menu with an option to use either (\u0026ldquo;base\u0026rdquo; uses read.csv and \u0026ldquo;readr\u0026rdquo; uses read_csv) From the Rstudio Blog about the readr package:\nCompared to base equivalents like read.csv, readr is much faster and gives more convenient output: it never converts strings to factors, can parse date/times, and it doesn’t munge the column names.\nGreat! If you have dates and times in a column, its possible that read_csv will see this and then correctly format them as dates. So you wouldn’t need to do this yourself (covered in Appendix of Chapter 2 of the Second Edition).\nSomething else you might notice is that the data appears different if you look at it in the Console. This is because read_csv brings the data in as a special type of object called a tibble. (The standard read.csv function bring the data in as a standard data.frame.) You can read all about tibbles on the RStudio blog. Why are they better than data.frames? First, they only show the first ten lines and as many variables as will comfortably fit when you look at them in the Console, and the type of variable is given, and the number of additional rows and variables not displayed. E.g.:\nlibrary(readr) dd \u0026lt;- read_csv(readr_example(\u0026quot;mtcars.csv\u0026quot;))  ## Parsed with column specification: ## cols( ## mpg = col_double(), ## cyl = col_double(), ## disp = col_double(), ## hp = col_double(), ## drat = col_double(), ## wt = col_double(), ## qsec = col_double(), ## vs = col_double(), ## am = col_double(), ## gear = col_double(), ## carb = col_double() ## )  dd  ## # A tibble: 32 x 11 ## mpg cyl disp hp drat wt qsec vs am gear carb ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 21 6 160 110 3.9 2.62 16.5 0 1 4 4 ## 2 21 6 160 110 3.9 2.88 17.0 0 1 4 4 ## 3 22.8 4 108 93 3.85 2.32 18.6 1 1 4 1 ## 4 21.4 6 258 110 3.08 3.22 19.4 1 0 3 1 ## 5 18.7 8 360 175 3.15 3.44 17.0 0 0 3 2 ## 6 18.1 6 225 105 2.76 3.46 20.2 1 0 3 1 ## 7 14.3 8 360 245 3.21 3.57 15.8 0 0 3 4 ## 8 24.4 4 147. 62 3.69 3.19 20 1 0 4 2 ## 9 22.8 4 141. 95 3.92 3.15 22.9 1 0 4 2 ## 10 19.2 6 168. 123 3.92 3.44 18.3 1 0 4 4 ## # … with 22 more rows  In summary, everything in the Second Edition will work just fine, but what you see in the Console might look a little different at times.\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"556d6b5c82b2f5857f44098c889f422a","permalink":"/posts/importing-data-update/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/posts/importing-data-update/","section":"posts","summary":"As the Second Edition of Getting Started with R was going to press, Rstudio changed the function it uses to import data in the Import Dataset tool, from the base function read.csv() to the read_csv() function in the readr package. Since then, the Import Dataset button gives a menu with an option to use either (\u0026ldquo;base\u0026rdquo; uses read.csv and \u0026ldquo;readr\u0026rdquo; uses read_csv) From the Rstudio Blog about the readr package:","tags":[],"title":"Importing data update","type":"posts"},{"authors":null,"categories":null,"content":" Datasets used in Getting Started with R, Second Edition All the datasets (and more) one zip file.\nWe don\u0026rsquo;t any longer distribute datasets for the first edition of Getting Started with R.\n","date":1536444000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1536444000,"objectID":"5c829381d43380f433483ca88f85e148","permalink":"/books/datasets/","publishdate":"2018-09-09T00:00:00+02:00","relpermalink":"/books/datasets/","section":"books","summary":"Datasets used in Getting Started with R, Second Edition All the datasets (and more) one zip file.\nWe don\u0026rsquo;t any longer distribute datasets for the first edition of Getting Started with R.","tags":null,"title":"Datasets","type":"docs"},{"authors":null,"categories":null,"content":"   We are pleased to announce the second edition of Getting Started with R Paperback at OUP, Amazon UK, Amazon.com, Amazon.de, Buch.ch\nElectric version at Oxford Scholarship Online.\nWe got great feedback about our book \u0026ldquo;Getting Started with R, An Introduction for Biologists\u0026rdquo;, but realised over the last couple of years that we were using and teaching R very differently from what we wrote about back in 2010. So we have almost completely rewritten for a very different and new Second Edition.\n Now based on using RStudio Now based around dplyr and ggplot2 for data management and grahics New basic examples for regression, 1-way ANOVA and 2-way ANOVA A whole new chapter on the Generalised Linear Model That same Getting Started with R attitude.  While many of the tools and functions have changed in this new edition, the content remains clear and enjoyable. We still walk your through, in that R4All style, from the basics of setting up your computer, through to data import and exploration and ultimately to a series of statistical models for interpreting data and experiments.\nWhat is so different and new? Several recent developments in the R community have made it even easier for us to help you focus more on your data and questions. RStudio has emerged as a brilliant cross-platform interface to working with R. Furthermore, Hadley Wickham and colleagues, have developed several add-on packages including as dplyr, tidyr and ggplot2 that not only provide consistent and intuitive ways to work with your data, but are emerging as industry standards (academic and non-academic).\nWe couldn\u0026rsquo;t afford to ignore these developments. The second edition thus dispenses entirely with classic methods of using R, and instead embraces RStudio along with dplyr, tidyr and ggplot2.\nYou\u0026rsquo;ll love it.\nAnything else new? We have a new co-author, Dylan Childs. He better at using R than Owen and Andrew, and kind of grumpy, so fits in well. He\u0026rsquo;s also helped us expand our stats offerings in the book. Not only do we provide more examples for basic stats (Regression, 1-way ANOVA, 2-way ANOVA) but a whole new chapter on the Generalised Linear Model.\n","date":1536444000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1536444000,"objectID":"7df86136d0d40f7e3353479e39cf7820","permalink":"/books/gswr2/","publishdate":"2018-09-09T00:00:00+02:00","relpermalink":"/books/gswr2/","section":"books","summary":"We are pleased to announce the second edition of Getting Started with R Paperback at OUP, Amazon UK, Amazon.com, Amazon.de, Buch.ch\nElectric version at Oxford Scholarship Online.\nWe got great feedback about our book \u0026ldquo;Getting Started with R, An Introduction for Biologists\u0026rdquo;, but realised over the last couple of years that we were using and teaching R very differently from what we wrote about back in 2010. So we have almost completely rewritten for a very different and new Second Edition.","tags":null,"title":"Getting Started with R, Second Edition","type":"docs"},{"authors":null,"categories":null,"content":" Lost packages Here is a post Lost Packages about what to do if, when attempting to install a package you get a message like package ggfortify is not available (for R Version 3.2.4).\nImporting data update If you\u0026rsquo;re having a brain melt figuring out the difference between read_csv and read.csv and their use in the RStudio Import Dataset tool, look at this post about using the readr package and read_csv function to import data.\nAbout the first edition of Getting Started with R   “Blowing away any feeling of intimidation is what this book is about.” – Graeme Ruxton, Trends in Ecology Evolution.\nDesigned for undergraduates, grad students and staff, we show you how to import, explore, graph, and start analysing your data, keeping you focused on your ultimate goals.\n A simple, easy, and engaging introduction to R for biologists. Walks readers through the fundamentals of using R, from that first step of importing datathrough to managing and exploring it and ultimately producing figures and analyses Delivers an efficient, accurate, reliable, and reproducible workflow. We provide a simple, efficient, reliable, accurate, and reproducible workflow for you in an engaging and sometimes humorous format.  Getting Started with R – its the book you and your students want to read before all the others.\nWell, it was. Now you should only look at the Second Edition.\n","date":1536444000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1536444000,"objectID":"4eb2d8e54d17d72ffa553c8ce6589aac","permalink":"/books/miscellany/","publishdate":"2018-09-09T00:00:00+02:00","relpermalink":"/books/miscellany/","section":"books","summary":"Lost packages Here is a post Lost Packages about what to do if, when attempting to install a package you get a message like package ggfortify is not available (for R Version 3.2.4).\nImporting data update If you\u0026rsquo;re having a brain melt figuring out the difference between read_csv and read.csv and their use in the RStudio Import Dataset tool, look at this post about using the readr package and read_csv function to import data.","tags":null,"title":"Miscellany","type":"docs"}]