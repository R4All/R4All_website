[{"authors":null,"categories":null,"content":"Owen has also used R for nearly 20 years, and has particular expertise in teaching beginners, multivariate statistics, spatial data, programming, maximum likelihood estimation, and visualisation (i.e., nice graphs!). His research focuses on the causes and consequences of extinctions in a changing world. His group performs experiments with microbial communities, models the structure of food webs, analyses variation in biodiversity, and does fieldwork in Iceland, the UK, and Switzerland.\n","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"598b63dd58b43bce02403646f240cd3c","permalink":"https://r4all.org/author/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/admin/","section":"author","summary":"Owen has also used R for nearly 20 years, and has particular expertise in teaching beginners, multivariate statistics, spatial data, programming, maximum likelihood estimation, and visualisation (i.e., nice graphs!). His research focuses on the causes and consequences of extinctions in a changing world. His group performs experiments with microbial communities, models the structure of food webs, analyses variation in biodiversity, and does fieldwork in Iceland, the UK, and Switzerland.","tags":null,"title":"","type":"author"},{"authors":null,"categories":null,"content":"Andrew is an evolutionary ecologists. He is interested in how the environment affects organism traits and how this determines the distribution and abundance of species and the complexity and structure of their communities. He\u0026rsquo;s been using R for more than 20 years and specialises in enthusiastically explaining things three or four different ways. He still likes learning to use new methods and tools. He also rides bicycles a fair bit.\n","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"c22df01f6f5d728d9ffcb11323d417b6","permalink":"https://r4all.org/author/andrew/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/andrew/","section":"author","summary":"Andrew is an evolutionary ecologists. He is interested in how the environment affects organism traits and how this determines the distribution and abundance of species and the complexity and structure of their communities. He\u0026rsquo;s been using R for more than 20 years and specialises in enthusiastically explaining things three or four different ways. He still likes learning to use new methods and tools. He also rides bicycles a fair bit.","tags":null,"title":"","type":"author"},{"authors":null,"categories":null,"content":"Dylan has also used R for nearly 20 years, and has particular expertise in teaching beginners, additive models, numerical simulation techniques, integral projection models, and data visualisation. His research focuses on the environmental and demographic drivers of population dynamics and selection. His group models all kinds of organisms, from arable weeds such as black grass to the Soay sheep of St Kilda.\n","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"b67a896c1bc60973e821a3db923e6ab2","permalink":"https://r4all.org/author/dylan/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/dylan/","section":"author","summary":"Dylan has also used R for nearly 20 years, and has particular expertise in teaching beginners, additive models, numerical simulation techniques, integral projection models, and data visualisation. His research focuses on the environmental and demographic drivers of population dynamics and selection. His group models all kinds of organisms, from arable weeds such as black grass to the Soay sheep of St Kilda.","tags":null,"title":"","type":"author"},{"authors":null,"categories":null,"content":"Natalie Cooper\u0026rsquo;s reasearch aims to understand broad-scale patterns of biodiversity. She uses cutting-edge phylogenetic comparative methods and various large datasets to investigate a variety of topics using R. She currently has projects on whales, snakes, dinosaurs, bryozoans, corals, pangolins, tenrecs, fishes, chameleons, amphibians and many more! Natalie is an advocate for diversity in STEM and reproducibility. In her spare time she likes to read, watch Netflix and climb mountains, while sticking it to the patriarchy.\n","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"3c08f1613c62381704970378bad25944","permalink":"https://r4all.org/author/natalie/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/natalie/","section":"author","summary":"Natalie Cooper\u0026rsquo;s reasearch aims to understand broad-scale patterns of biodiversity. She uses cutting-edge phylogenetic comparative methods and various large datasets to investigate a variety of topics using R. She currently has projects on whales, snakes, dinosaurs, bryozoans, corals, pangolins, tenrecs, fishes, chameleons, amphibians and many more! Natalie is an advocate for diversity in STEM and reproducibility. In her spare time she likes to read, watch Netflix and climb mountains, while sticking it to the patriarchy.","tags":null,"title":"","type":"author"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"d41d8cd98f00b204e9800998ecf8427e","permalink":"https://r4all.org/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"author","summary":"","tags":null,"title":"Authors","type":"author"},{"authors":null,"categories":null,"content":"Please look in the menu on the left (or elsewhere) for information about the book Getting Started with R, the datasets used in the book, and some miscellany.\n","date":1536444000,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536444000,"objectID":"b905e45f7194913eeeed179620391d24","permalink":"https://r4all.org/books/","publishdate":"2018-09-09T00:00:00+02:00","relpermalink":"/books/","section":"books","summary":"Please look in the menu on the left (or elsewhere) for information about the book Getting Started with R, the datasets used in the book, and some miscellany.","tags":null,"title":"Overview","type":"docs"},{"authors":null,"categories":null,"content":"","date":1461103200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461103200,"objectID":"3769b896f8c2f656f609ef66d8bfd1f5","permalink":"https://r4all.org/who-are-we/owen/","publishdate":"2016-04-20T00:00:00+02:00","relpermalink":"/who-are-we/owen/","section":"who-are-we","summary":"","tags":null,"title":" ","type":"who-are-we"},{"authors":null,"categories":null,"content":"","date":1461103200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461103200,"objectID":"0bfeec153670ac4794874b16bf7f9d76","permalink":"https://r4all.org/who-are-we/natalie/","publishdate":"2016-04-20T00:00:00+02:00","relpermalink":"/who-are-we/natalie/","section":"who-are-we","summary":"","tags":null,"title":" ","type":"who-are-we"},{"authors":null,"categories":null,"content":"","date":1461103200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461103200,"objectID":"102bb2725d25d84db6b1d3f0146dd443","permalink":"https://r4all.org/who-are-we/andrew/","publishdate":"2016-04-20T00:00:00+02:00","relpermalink":"/who-are-we/andrew/","section":"who-are-we","summary":"","tags":null,"title":" ","type":"who-are-we"},{"authors":null,"categories":null,"content":"","date":1461103200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461103200,"objectID":"7f683e292b0f5fa1e163249c15004108","permalink":"https://r4all.org/who-are-we/dylan/","publishdate":"2016-04-20T00:00:00+02:00","relpermalink":"/who-are-we/dylan/","section":"who-are-we","summary":"","tags":null,"title":" ","type":"who-are-we"},{"authors":null,"categories":null,"content":"  Sometimes, in the second edition, we use the levels function to get the unique levels of a variable. For example on page 133 we do levels(growth.moo$diet) to get the unique levels of the diet variable. Today, this does not work. Below I explain why and how to fix it. Short version is use unique instead of levels or convert the variables to factors.\nPrepare We will use the mutate function from the dplyr package, so please ensure you have that package installed.\n Import the data In the next line of code I import the data from github, rather than a local copy. This saves us having to deal with local location of the data file. I would normally work with a local copy, however.\ngrowth.moo \u0026lt;- read.csv(url(\u0026quot;https://raw.githubusercontent.com/r4all/datasets/master/growth.csv\u0026quot;))  Using unique rather than levels Looking at the structure of the data in R we see:\nstr(growth.moo) ## \u0026#39;data.frame\u0026#39;: 48 obs. of 3 variables: ## $ supplement: chr \u0026quot;supergain\u0026quot; \u0026quot;supergain\u0026quot; \u0026quot;supergain\u0026quot; \u0026quot;supergain\u0026quot; ... ## $ diet : chr \u0026quot;wheat\u0026quot; \u0026quot;wheat\u0026quot; \u0026quot;wheat\u0026quot; \u0026quot;wheat\u0026quot; ... ## $ gain : num 17.4 16.8 18.1 15.8 17.7 ... Supplement and diet are both chr (character) type variables.\nHence the levels function doesn’t give us the levels. Instead, we get NULL:\nlevels(growth.moo$supplement) ## NULL levels(growth.moo$diet) ## NULL So, instead use unique:\nunique(growth.moo$supplement) ## [1] \u0026quot;supergain\u0026quot; \u0026quot;control\u0026quot; \u0026quot;supersupp\u0026quot; \u0026quot;agrimore\u0026quot; unique(growth.moo$diet) ## [1] \u0026quot;wheat\u0026quot; \u0026quot;oats\u0026quot; \u0026quot;barley\u0026quot; Awesomeness!\n Converting to a factor Another option is to convert the chr type variables to be factor type variables. There are many ways to achieve this, here are two.\nIf we want to convert to factors all the chr variables in our data, then we can use the type.convert function with the argument as.is = FALSE. Making this FALSE tells the type.convert function to not keep character type variables as they are, but rather to convert them to factors.\ngrowth.moo.factors1 \u0026lt;- type.convert(growth.moo, as.is = FALSE) str(growth.moo.factors1) ## \u0026#39;data.frame\u0026#39;: 48 obs. of 3 variables: ## $ supplement: Factor w/ 4 levels \u0026quot;agrimore\u0026quot;,\u0026quot;control\u0026quot;,..: 3 3 3 3 2 2 2 2 4 4 ... ## $ diet : Factor w/ 3 levels \u0026quot;barley\u0026quot;,\u0026quot;oats\u0026quot;,..: 3 3 3 3 3 3 3 3 3 3 ... ## $ gain : num 17.4 16.8 18.1 15.8 17.7 ... Great! What were character type variables are now factors.\nBy the way, the default since R 4.0.0 is as.is = TRUE which can be understood as keep variables as they are–do not convert them to factors. We wrote the second edition before 4.0.0, and this is why levels worked when we wrote the second edition, but does not work now.\nAnother way is to individually convert each variable, for example:\ngrowth.moo.factors2 \u0026lt;- dplyr::mutate(growth.moo, supplement = as.factor(supplement), diet = as.factor(diet)) str(growth.moo.factors2) ## \u0026#39;data.frame\u0026#39;: 48 obs. of 3 variables: ## $ supplement: Factor w/ 4 levels \u0026quot;agrimore\u0026quot;,\u0026quot;control\u0026quot;,..: 3 3 3 3 2 2 2 2 4 4 ... ## $ diet : Factor w/ 3 levels \u0026quot;barley\u0026quot;,\u0026quot;oats\u0026quot;,..: 3 3 3 3 3 3 3 3 3 3 ... ## $ gain : num 17.4 16.8 18.1 15.8 17.7 ... Awesomeness 2!\nThanks for reading. Have a nice day!\n ","date":1662768000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1662768000,"objectID":"82e696e0adfca325f789b501c91392fe","permalink":"https://r4all.org/posts/2022-09-10-levels-not-working-in-2nd-edition-of-getting-started/","publishdate":"2022-09-10T00:00:00Z","relpermalink":"/posts/2022-09-10-levels-not-working-in-2nd-edition-of-getting-started/","section":"posts","summary":"Sometimes, in the second edition, we use the levels function to get the unique levels of a variable. For example on page 133 we do levels(growth.moo$diet) to get the unique levels of the diet variable. Today, this does not work. Below I explain why and how to fix it. Short version is use unique instead of levels or convert the variables to factors.\nPrepare We will use the mutate function from the dplyr package, so please ensure you have that package installed.","tags":null,"title":"levels() not working in 2nd Edition of Getting Started","type":"posts"},{"authors":null,"categories":[],"content":"In our book Insights we help readers learn how to use the amazing ggplot2 package to make visualisations. If you have some experience with ggplot2, you may think our method of teaching it, and of using it in the book are a bit odd. Here we explain our reason for teaching it the way we teach it.\nFor example, here is the code for the first graph we make:\nbats_Age_Sex %\u0026gt;% ggplot() + geom_col(mapping = aes(x=Sex, y=num_bat_IDs, fill=Age))  And if we had not piped the dataset into ggplot then we would have done this:\nggplot() + geom_col(data = bats_Age_Sex, mapping = aes(x=Sex, y=num_bat_IDs, fill=Age))  Generally speaking, we first teach students to put all the arguments into the geom. We believe this is a valuable didactic tool/approach (and also not bad to do in any case). This is because we believe that it\u0026rsquo;s important to know how the geoms work and what they need, and that this is best seen by specifying the necessary information in the geom.\nAfter we\u0026rsquo;ve explained and practiced this approach numerous times with different geoms, we explain how inheritance from the ggplot function works, and when it is useful.\n","date":1608076800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1608076800,"objectID":"d7fcb5864a97fc009178f6e636929b43","permalink":"https://r4all.org/posts/2020-12-16-how-we-teach-ggplot-in-insights/","publishdate":"2020-12-16T00:00:00Z","relpermalink":"/posts/2020-12-16-how-we-teach-ggplot-in-insights/","section":"posts","summary":"In our book Insights we help readers learn how to use the amazing ggplot2 package to make visualisations. If you have some experience with ggplot2, you may think our method of teaching it, and of using it in the book are a bit odd. Here we explain our reason for teaching it the way we teach it.\nFor example, here is the code for the first graph we make:\nbats_Age_Sex %\u0026gt;% ggplot() + geom_col(mapping = aes(x=Sex, y=num_bat_IDs, fill=Age))  And if we had not piped the dataset into ggplot then we would have done this:","tags":[],"title":"How we teach ggplot (in the Insights book)","type":"posts"},{"authors":null,"categories":null,"content":"An Introduction for the Life and Environmental Sciences.\n  We are pleased to announce our brand new book, Insights from Data with R, An Introduction for the Life and Environmental Sciences. We call it Insights for short.\nIt is published by OUP and available from the OUP website.\nTable of contents: Here is the Table of Contents (a pdf).\nPreface: Here is the Preface section (a pdf).\nInsights is designed and written for undergraduates in the life and environmental sciences. It assumes no knowledge of statistics, data or programming.\nInsights is intended as a go-to resource to accompany undergraduates during their first experience of working with data to get answers.\nUnique/important features of Insights include:\n Based on very well received undergraduate data analysis courses taught by the authors. Students work along with data from real scientific studies published on data repositories. Has four detailed Workflow Demonstrations. Focuses on developing solid foundations of data management, summarising, and visualisation, to get robust and reproducible insights. Intentionally leaves statistical tests to subsequent courses/books. Uses RStudio and the tidyverse.  For more information about the book, please check out the companion website, where there are extensive additional resources (including three of the four Workflow Demonstrations).\nOur other book Getting Started with R, An Introduction for Biologists is for people that have already some experience with data analysis, and that want to learn or improve their R competence and confidence.\n","date":1608073200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1608073200,"objectID":"ea19b2cca626b0f03a8e7d17f2b36b5c","permalink":"https://r4all.org/books/insights/","publishdate":"2020-12-16T00:00:00+01:00","relpermalink":"/books/insights/","section":"books","summary":"An Introduction for the Life and Environmental Sciences.\n  We are pleased to announce our brand new book, Insights from Data with R, An Introduction for the Life and Environmental Sciences. We call it Insights for short.\nIt is published by OUP and available from the OUP website.\nTable of contents: Here is the Table of Contents (a pdf).\nPreface: Here is the Preface section (a pdf).\nInsights is designed and written for undergraduates in the life and environmental sciences.","tags":null,"title":"Insights from Data with R.","type":"docs"},{"authors":null,"categories":[],"content":"[This is a minimal post due to very limited time.]\nWe need to check the assumptions of our linear model (e.g. regression, ANOVA, ANCOVA) are not too badly violated. We often use four diagnostic graphs to do so. One of these shows standardised residuals plotted against leverage (each observation has a value).\nThe take home message of this post is if your model contains at least one continuous explanatory variable, use the base R methods for making your diagnostic plots:\npar(mfrow = c(2,2)) plot(my_model)  Here is an example that looks at relationship between earthworm mass (response variable) and two explanatory variables (species ID of the earthworm, and stomach circumference of the earthworm).\nPrepare R:\nlibrary(tidyverse) library(ggfortify)  Read the data and make English variable names:\nworms \u0026lt;- read_csv(\u0026quot;https://raw.githubusercontent.com/opetchey/BIO144/master/3_datasets/earthworm.csv\u0026quot;) %\u0026gt;% rename(Mass = Gewicht, Species = Gattung, Stomach_circum = Magenumf)  ## Parsed with column specification: ## cols( ## Gattung = col_character(), ## Nummer = col_double(), ## Gewicht = col_double(), ## Fangdatum = col_character(), ## Magenumf = col_double() ## )  Plot and inspect the data:\nworms %\u0026gt;% ggplot() + geom_point(mapping = aes(x = Stomach_circum, y = Mass, col = Species))  From this, we expect to see evidence of non-linearity in the diagnostic plot of residuals against fitted values (but this does not concern the issue addressed in this post).\nNow make the model including both explanatory variables and no interaction between them:\nmod_sp_circ_noint \u0026lt;- lm(Mass ~ Stomach_circum + Species, data = worms)  Here is the base R method for making the four model diagnostic plots:\npar(mfrow=c(2,2)) plot(mod_sp_circ_noint)  Take note of the plot of standardised residuals versus leverage.\nNow compare to the same produced by the autoplot function of the ggfortify package:\nautoplot(mod_sp_circ_noint)  It seems that the autoplot function in the ggfortify package is not doing what we would like and expect\u0026hellip; there is a continuous explanatory variable, so leverage is not constant, and it should make a graph with leverage on the x-axis.\nCompare this difference in behaviour between base R and ggfortify::autoplot when there is only a continuous explanatory variable in the model:\nmod_circ \u0026lt;- lm(Mass ~ Stomach_circum, data = worms)  par(mfrow=c(2,2)) plot(mod_circ)  autoplot(mod_circ)  And when there is only a factor variable:\nmod_spp \u0026lt;- lm(Mass ~ Species, data = worms)  par(mfrow=c(2,2)) plot(mod_spp)  autoplot(mod_spp)  I believe it is useful to have the residuals versus leverage plot if there is continuous explanatory variable, so would then use the base R method to make the diagnostic plots.\n","date":1584576000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1584576000,"objectID":"508146ee57ca7701ab7054f1b27370ca","permalink":"https://r4all.org/posts/diagnostic-plots-of-models-with-categotical-explanatory-variables/","publishdate":"2020-03-19T00:00:00Z","relpermalink":"/posts/diagnostic-plots-of-models-with-categotical-explanatory-variables/","section":"posts","summary":"[This is a minimal post due to very limited time.]\nWe need to check the assumptions of our linear model (e.g. regression, ANOVA, ANCOVA) are not too badly violated. We often use four diagnostic graphs to do so. One of these shows standardised residuals plotted against leverage (each observation has a value).\nThe take home message of this post is if your model contains at least one continuous explanatory variable, use the base R methods for making your diagnostic plots:","tags":[],"title":"Diagnostic plots of linear models with categotical explanatory variables","type":"posts"},{"authors":null,"categories":[],"content":"Inspiration for the following from from Richard McElreath\u0026rsquo;s Statistical Rethinking book, and some of the code comes from here: https://bookdown.org/ajkurz/Statistical_Rethinking_recoded/multivariate-linear-models.html#masked-relationship\nLet us think about the question of how the response variable y is related to two explanatory variables x1 and x2.\nFirst we make a dataset in which we know the relationships because we specify them: we make y = x1 - x2. Before this, we create x1 and x2 and make them correlated\u0026hellip;\nrm(list=ls())  library(tidyverse) library(patchwork) set.seed(141) # setting the seed makes the results reproducible N \u0026lt;- 100 # number of observations rho \u0026lt;- .8 # correlation between x_pos and x_neg dd \u0026lt;- tibble(x1 = rnorm(N), x2 = rnorm(N, rho*x1, sqrt(1 - rho^2)), y = rnorm(N, x1 - x2))  A quick look at the dataset\u0026hellip; three numeric \u0026lt;dbl\u0026gt; variables.\nglimpse(dd)  ## Observations: 100 ## Variables: 3 ## $ x1 \u0026lt;dbl\u0026gt; 0.51435972, -0.11277738, 0.06434006, -0.65524480, 0.50172420, -0.8… ## $ x2 \u0026lt;dbl\u0026gt; 0.929913616, 0.824215658, -0.060507248, -0.182433894, 1.952874568,… ## $ y \u0026lt;dbl\u0026gt; -1.927385720, -0.028922502, 0.405574747, 0.629677348, -2.459065811…  Figure 1 shows the three scatterplots on can make. Importantly, we see little evidence of the relationship between y and x1, or between y and x2 that we know exist. We can clearly see the correlation between the two explanatory variables x1 and x2.\nFigure 1: (a and b) Little evidence of a relationship between the response variable *y* and either of the two explanatory variables *x1*, or *x2*. (c) Strong correlation between the two explanatory variables *x1* and *x2*\n To reveal the relationship between y and x1 we need to control for the variation in x2. One way to do this is to divide the data into subsets in each of which there is relatively little variation in x2. With the following code we add a variable to the data set that contains categories of variation in x2. I.e. we cut the variation in x2 into 10 groups, and put the names of these groups in a new variable:\ndd \u0026lt;- mutate(dd, x2_cut = cut(x2, 10))  Figure 2, in particular the panels with more observations, clearly shows the positive relationships that we know exist. Great! Have a go at making an analogous plot while controlling for variation in x1. (By the way, we have more data in the middle, because x2 is normally distriuted.)\nFigure 2: The positive relationship between the response variable (*y*) and one of the explanatory variables (*x1*) is visible because each facet shows a relatively small range of variation in the other explanatory variable (*x2*).\n Another way to control for variation in each explanatory variable while \u0026ldquo;viewing\u0026rdquo; the relationship of the response variable with the other is multiple regression. Below we see very strong evidence of the positive relationship between y and x1 and negative between y and x2, and that estimated coefficients (slopes) are not different from the real ones (1). Whereas the univariate regression show much weaker evidence of any relationships, and estimated coefficients are poorly estimated.\nMultiple regression is, in effect, doing what we did when we cut up the data and plotted parts of it that contained little variation in the other variable.\nmod_x1x2 \u0026lt;- lm(y ~ x1 + x2, data = dd) summary(mod_x1x2)  ## ## Call: ## lm(formula = y ~ x1 + x2, data = dd) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.27190 -0.64723 -0.04082 0.68422 2.73434 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 0.07772 0.09878 0.787 0.433 ## x1 1.13521 0.15776 7.196 1.31e-10 *** ## x2 -1.26254 0.16259 -7.765 8.44e-12 *** ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## Residual standard error: 0.983 on 97 degrees of freedom ## Multiple R-squared: 0.3996,\tAdjusted R-squared: 0.3872 ## F-statistic: 32.27 on 2 and 97 DF, p-value: 1.801e-11  mod_x1 \u0026lt;- lm(y ~ x1, data = dd) summary(mod_x1)  ## ## Call: ## lm(formula = y ~ x1, data = dd) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.02457 -0.72337 0.00238 0.79139 2.95465 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 0.01596 0.12473 0.128 0.898 ## x1 0.21467 0.13186 1.628 0.107 ## ## Residual standard error: 1.245 on 98 degrees of freedom ## Multiple R-squared: 0.02633,\tAdjusted R-squared: 0.0164 ## F-statistic: 2.65 on 1 and 98 DF, p-value: 0.1067  mod_x2 \u0026lt;- lm(y ~ x2, data = dd) summary(mod_x2)  ## ## Call: ## lm(formula = y ~ x2, data = dd) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.79342 -0.95557 -0.02311 0.94306 2.39910 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 0.06106 0.12167 0.502 0.61689 ## x2 -0.38332 0.13218 -2.900 0.00461 ** ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## Residual standard error: 1.211 on 98 degrees of freedom ## Multiple R-squared: 0.07904,\tAdjusted R-squared: 0.06964 ## F-statistic: 8.41 on 1 and 98 DF, p-value: 0.004606  ","date":1584316800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1584316800,"objectID":"fe128886912311a92c9adc1f6236b198","permalink":"https://r4all.org/posts/lurking-variables-and-hidden-relationships/","publishdate":"2020-03-16T00:00:00Z","relpermalink":"/posts/lurking-variables-and-hidden-relationships/","section":"posts","summary":"Inspiration for the following from from Richard McElreath\u0026rsquo;s Statistical Rethinking book, and some of the code comes from here: https://bookdown.org/ajkurz/Statistical_Rethinking_recoded/multivariate-linear-models.html#masked-relationship\nLet us think about the question of how the response variable y is related to two explanatory variables x1 and x2.\nFirst we make a dataset in which we know the relationships because we specify them: we make y = x1 - x2. Before this, we create x1 and x2 and make them correlated\u0026hellip;","tags":[],"title":"Lurking variables and hidden relationships","type":"posts"},{"authors":null,"categories":[],"content":"This is the first in a series of posts about maximum likelihood methods for fitting statistical models to data. Inspiration for the material comes in large part from Drew Purves who presented something similar. Owen is using Drew’s approach as the basis for this course. Much of the R specific stuff is heavily influenced by Ben Bolker’s excellent book: Ecological Models and Data in R. The goal of this and the following posts includes: learning how to fit to our data more mechanistic models of arbitrary complexity. learning how to do this with ease, confidence, and complete transparency. at some point delving into robust and efficient parameter estimation methods (e.g., MCMC). at some point working out how to switch between frequentist and Bayesian approaches with ease. (Please note that the focus of this post is learning about maximum likelihood methods. R is a only a tool to help that learning, so we avoid putting lots of potentially distracting R code in the post, and rather make it available as a separate file.) Lets start on familiar ground by consider a common statistical approach, linear regression. This involves fitting a model (equation) to the observed data. The equation is:\n$$ y = a + bx + norm(0, \\sigma) $$\n $ y $ is the observed response variable $ x $ is the explanatory variable $ a $ is the intercept $ b $ is the slope $ norm(0, \\sigma) $ is the error term.  This is the model we fit to our data. We are trying to find the value of $ a $, $ b $, and $ \\sigma $ that fit the data. These are the parameters of the model. How do we find out the values of a and b that give the best regression line? Regression does so by minimizing the sum of squares, which is why we often use the term ‘least squares regression’. We are going to learn how to estimate the parameters of a model by maximising likelihood (instead of minimising least squares). We will start by considering a dataset and model even simpler than linear regression. This example may seem rather trivial. It is. However, better to start simple, with an example adequate to introduce many of the fundamental concepts we need.\nAssume we’ve counted the number of individuals in seven replicate quadrats that we’ve placed randomly in a field (don’t ask me why seven quadrats – maybe we left the other three in the lab – who knows). This is our observed data (seven numbers).\nNow, the definition of likelihood is $ p(data | model) $\n $ p $ stands for probability * data * is the observed data $ | $ (the vertical bar) can be read as ‘given the’ $ model $ = whatever model we like.  That is, $ p(data | model) $ can be read as “the probability of the data given the model”.\nIn our current example we will assume that individuals in the field are randomly distributed, and therefore that the count in each quadrat comes from a Poission distribution. The Poisson distribution has only one pa- rameter, the mean. This is what we want to estimate. Since this is a trivial example, we can easily find the most likely value of the mean by taking the arithmetic average (mean) of the observed values. We’re not go- ing to do this yet, since the purpose of using this example is illustration of concepts.\nSo, our model is $ Y_i = Poisson(M) $, where $ Y_i $ is the ith (pronounced eye–eth) observation of the response variable (here the number of individuals counted in the first quadrat) and $ M $ is the mean of the Poisson distribution – this is what we want to estimate.\nWe want to find the likelihood – $ p(data | model) $ – and so first we calculate the likelihood of each individual observation. The likelihood of the ith observation is,\n$$ p(Y_i | M) $$\nThis is the probability of getting the value $ Y_i $ given the mean $ M $. So if the first count $ Y_1 = 6 $ and we guess a mean of 7, we can find the probability of $ Y_1 = 6 $ in Excel with: Poisson(6, 7, FALSE) (why false?). Or in R using dpois(6, 7)\nIf you do either of these you will find that the probability of observing a 6 given a Poisson distribution with mean 7 is about 0.15.\nWe do this for each value of the response variable, log the probabilities, and then add them up. Lets do this in Excel, using the worksheet ‘eg1’ in this Excel spreadsheet. Here is a screen grab of the worksheet:\n    The first column (from row 4 down) is the data, the second is the probability of each data point given the mean (look at the formula to see the call to the Excel Possion function). The third column is the log (base e; ln) of the probabilities (i.e., the log-likelihood. Often this is just called the likelihood, but we should try to always use the term log-likelihood, for accuracy. At the bottom of column C is the sum of the log-likelihoods. Cell A2 contains our guess of the mean of the observed data.\n Look at the formulas in the cells to check they are what you expect. Try changing the guess of the mean and see what happens to the sum of the log probabilities.  By changing the guess of the mean (cell A2 in the worksheet ‘eg1’) you change the log-likelihood. The challenge is to find the value of the mean that will minimise the log-likelihood (= maximise the likelihood). This value (estimate) of the mean is called the maximum likelihood estimate. We are interested in maximising the likelihood, or equivalently, minimising it (make as close to zero as possible). The more negative the sum of log likelihood the worse the model (guess of the mean) is. (Read this paragraph while playing with the Excel worksheet, until you are sure you get it. It is very important.)\n(By the way, we log the probabilities for a few reasons, including that computers often have problems deal- ing with very small numbers.)\nBy playing with the guess of the mean in cell A2 of the Excel worksheet, you may have found that a value close to 6 minimises the log-likelihood (makes it closest to zero, i.e., least negative). You have just obtained a maximum likelihood estimate of the parameter of this model.\nNow take a look at the R script file eg1.r. This shows how you can do with R what you just did with Excel.\nOf course, we don’t want to have to guess the mean, look at the log-likelihood, guess again, and look at the log-likelihood again, and so on. We want the computer to do the work. In the R file is some script that makes a vector of guesses of the mean, loops through these, and records the log-likelihood. Then we can plot the value of the log-likelihood versus the guess of the mean.\n    The maximum of this curve is the maximum likelihood. It is achieved when the guess of the mean has the val- ue of 6.1, where the vertical dashed line is drawn. The log-likelihood here is -14.821. This curve is also know as the likelihood profile.\nHow did we do? Our guess was 6.1 and the actual mean is 6.143, with a log-likelihood of -14.820. Not bad then! Question for you: What should we change in the R script to get a more accurate estimate of the mean? Now, this is important. If we suspected our observed data were poisson distributed, and we wanted to mod- el that data in R, we might easily decide to use the generalised linear model funtion glm and specify a pois- son distribution (family=poisson). The code to do this would look something like, glm(n ~ 1, family=poisson) where n is the response variable that here is the number of individuals in each of the quadrats.\nThis model is in the R script previously mentioned, and the coefficient (estimated parameter / mean) is 1.8153. Not the mean that we found (6.1). This is because specifying a poisson distribution (family=poisson) means that a log link function is used. So we have to un-log the coefficient to get the actual value\u0026hellip; exp(1.8153) = 6.1428. Good.\nWhat else can we do with this simple example? Let’s figure out for ourselves the AIC (Akaike Information Cri- teria) of our model. The definition of AIC is,\n$$ AIC = -2*L + 2*n $$\nWhere L is the log-likelihood and p is the number of parameters (only one, the mean, in our model). Our log-likelihood was -14.82117, so AIC = 31.64. The AIC given by the glm function is 31.64 also. Great.\nHow does glm work? How does it find the mean? It tries many guesses of the mean and sees which gives the maximum likelihood, just like we did in the R script that made the graph above. However, glm uses a search method that is a bit smarter than ours (I think it calls the function optim which by default uses the Nelder-Mead search algorith, but this is not too important now). What is important is to realise that glm is just searching around the parameter space looking for a maximum likelihood. It tries to go uphill on the likelihood profile. When it can’t go uphill any more, it knows it’s reached the maximum likelihood. You should now be able to have a good go at answering these questions:\n What are parameters? What is a variable? What is likelihood? What does this mean p(data| model)? How can we maximise likelihood. What is the maximum likelihood estimate of a parameter? What is a likelihood profile?  ","date":1551225600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1551225600,"objectID":"8a4f23d9bbf83a64e17351267ae46fd1","permalink":"https://r4all.org/posts/maximum-likelihood-part-1/","publishdate":"2019-02-27T00:00:00Z","relpermalink":"/posts/maximum-likelihood-part-1/","section":"posts","summary":"This is the first in a series of posts about maximum likelihood methods for fitting statistical models to data. Inspiration for the material comes in large part from Drew Purves who presented something similar. Owen is using Drew’s approach as the basis for this course. Much of the R specific stuff is heavily influenced by Ben Bolker’s excellent book: Ecological Models and Data in R. The goal of this and the following posts includes: learning how to fit to our data more mechanistic models of arbitrary complexity.","tags":[],"title":"Maximum likelihood part 1","type":"posts"},{"authors":null,"categories":[],"content":"Just a little demo of what happens if you don’t or do adjust your r-squared. Here’s the bottom line\u0026hellip;\nAs we increase the number of explanatory variables in a linear model (e.g. multiple regression) the unadjusted r-squared increaes (green dots) even if the additional explanatory variables contain only random numbers. The adjusted r-squared is \u0026ldquo;adjusted\u0026rdquo; so it does not! So if we simply want to know the proportion of variance explained by our model we are fine using the unadjusted r-squared. If, however, we want to compare the r-squared of models with different numbers of explanatory variables, we should compare the adjusted r-squared.\nHere\u0026rsquo;s the code for making the figure. (Done before we converted to the tidyverse!)\n## Lets do some multiple regression, with different numbers of explanatory variables ## with completely random data numb.expl.vars \u0026lt;- floor(rep(2^seq(0, 5, 0.5), each=50)) ## Number of observations n \u0026lt;- 100 ## The response variable y \u0026lt;- rnorm(n) ## Function to return the unadjusted and adjusted r-squared get.r2 \u0026lt;- function(ne) { x \u0026lt;- as.data.frame(matrix(rnorm(n*ne), n, ne)) m1 \u0026lt;- lm(y ~ ., x) result \u0026lt;- c(summary(m1)$r.squared, summary(m1)$adj.r.squared) result } ## use lapply to run the function over the number of explanatory variables vec rez \u0026lt;- do.call(rbind, lapply(numb.expl.vars, function(x) get.r2(x))) ## get the mean r-squared and adjusted r-squared per number of expl varbs means \u0026lt;- aggregate(rez, list(numb.expl.vars=numb.expl.vars), mean) ## plot the data matplot(log2(numb.expl.vars), rez, type=\u0026quot;n\u0026quot;, ann=F, axes=F) box() abline(h=0) matpoints(jitter(log2(numb.expl.vars)), rez, pch=19, col=c(\u0026quot;#11ff1144\u0026quot;, \u0026quot;#ff111144\u0026quot;)) mtext(1, line=2.5, text=\u0026quot;Number of explanatory variables\u0026quot;) mtext(2, line=2, text=\u0026quot;R-squared\\n(green unadjusted, red adjusted)\u0026quot;) axis(1, at=0:5, labels=2^(0:5)) axis(2) matpoints(log2(means[,1]), means[,2], pch=21, bg=c(\u0026quot;#11ff1144\u0026quot;)) matpoints(log2(means[,1]), means[,3], pch=21, bg=c(\u0026quot;#ff111144\u0026quot;)) ## for fun, calculate the adjusted r-squared manually adj.rsquared \u0026lt;- 1 - (1-rez[,1])*(n-1)/(n-numb.expl.vars-1) sum(abs(adj.rsquared-rez[,2])\u0026gt;1e-10) ## should be zero  ","date":1551225600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1551225600,"objectID":"def77688abf7a41612350e17e9206f46","permalink":"https://r4all.org/posts/why-adjust-your-r-squared/","publishdate":"2019-02-27T00:00:00Z","relpermalink":"/posts/why-adjust-your-r-squared/","section":"posts","summary":"Just a little demo of what happens if you don’t or do adjust your r-squared. Here’s the bottom line\u0026hellip;\nAs we increase the number of explanatory variables in a linear model (e.g. multiple regression) the unadjusted r-squared increaes (green dots) even if the additional explanatory variables contain only random numbers. The adjusted r-squared is \u0026ldquo;adjusted\u0026rdquo; so it does not! So if we simply want to know the proportion of variance explained by our model we are fine using the unadjusted r-squared.","tags":[],"title":"Why adjust your r-squared","type":"posts"},{"authors":null,"categories":[],"content":"To use the functions in an add-on package you first need to install the package. Remember you only need install it once.\nDuring the writing of the book, and in early 2018 the normal method for installing the ggfortify add-on package didn’t work (we got the message package ggfortify is not available (for R Version 3.2.4)).\nThis has not happened for some time, so hopefully you won\u0026rsquo;t experience it. If you do\u0026hellip;\nMany packages are now developed in an environment / web service called GitHub, and R has an interface for installing packages directly from GitHub. This is true for ggfortify. Here is how you can access the package that way:\ninstall.packages(\u0026quot;devtools\u0026quot;) library(devtools) install_github('sinhrks/ggfortify')  Another option: if the packages was once on CRAN, and archived, older version may still be available. Probably you can find this by googling ggfortify cran and following links to a rather unfriendly looking web page that lists a few files, one of which was, at the time of writing this, gfortify_0.1.0.tar.gz. Download this to your computer, making sure you know which folder it gets downloaded into. Then, in Rstudio, Click the Install button in the Packages (as usual when you want to install a package). However, instead of Installing from: Repository CRAN choose Install from: Package Archive File (.tgz, .tar.gz). Then you need to click Browse and find the file you just downloaded. Then click Install. Hopefully it will still work with the version of R you have installed.\nAnother option is to do it the old fashioned way. E.g. instead of using the autoplot() function from the ggfortify package, do this:\npar(mfrow(2,2)) plot(model, add.smooth = FALSE)  ","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"e953d378ca03f910a4ca7f88b0cfa570","permalink":"https://r4all.org/posts/getting-lost-packages/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/posts/getting-lost-packages/","section":"posts","summary":"To use the functions in an add-on package you first need to install the package. Remember you only need install it once.\nDuring the writing of the book, and in early 2018 the normal method for installing the ggfortify add-on package didn’t work (we got the message package ggfortify is not available (for R Version 3.2.4)).\nThis has not happened for some time, so hopefully you won\u0026rsquo;t experience it. If you do\u0026hellip;","tags":[],"title":"Getting lost packages","type":"posts"},{"authors":null,"categories":[],"content":"As the Second Edition of Getting Started with R was going to press, Rstudio changed the function it uses to import data in the Import Dataset tool, from the base function read.csv() to the read_csv() function in the readr package. Since then, the Import Dataset button gives a menu with an option to use either (\u0026ldquo;base\u0026rdquo; uses read.csv and \u0026ldquo;readr\u0026rdquo; uses read_csv) From the Rstudio Blog about the readr package:\nCompared to base equivalents like read.csv, readr is much faster and gives more convenient output: it never converts strings to factors, can parse date/times, and it doesn’t munge the column names.\nGreat! If you have dates and times in a column, its possible that read_csv will see this and then correctly format them as dates. So you wouldn’t need to do this yourself (covered in Appendix of Chapter 2 of the Second Edition).\nSomething else you might notice is that the data appears different if you look at it in the Console. This is because read_csv brings the data in as a special type of object called a tibble. (The standard read.csv function bring the data in as a standard data.frame.) You can read all about tibbles on the RStudio blog. Why are they better than data.frames? First, they only show the first ten lines and as many variables as will comfortably fit when you look at them in the Console, and the type of variable is given, and the number of additional rows and variables not displayed. E.g.:\nlibrary(readr) dd \u0026lt;- read_csv(readr_example(\u0026quot;mtcars.csv\u0026quot;))  ## Parsed with column specification: ## cols( ## mpg = col_double(), ## cyl = col_double(), ## disp = col_double(), ## hp = col_double(), ## drat = col_double(), ## wt = col_double(), ## qsec = col_double(), ## vs = col_double(), ## am = col_double(), ## gear = col_double(), ## carb = col_double() ## )  dd  ## # A tibble: 32 x 11 ## mpg cyl disp hp drat wt qsec vs am gear carb ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 21 6 160 110 3.9 2.62 16.5 0 1 4 4 ## 2 21 6 160 110 3.9 2.88 17.0 0 1 4 4 ## 3 22.8 4 108 93 3.85 2.32 18.6 1 1 4 1 ## 4 21.4 6 258 110 3.08 3.22 19.4 1 0 3 1 ## 5 18.7 8 360 175 3.15 3.44 17.0 0 0 3 2 ## 6 18.1 6 225 105 2.76 3.46 20.2 1 0 3 1 ## 7 14.3 8 360 245 3.21 3.57 15.8 0 0 3 4 ## 8 24.4 4 147. 62 3.69 3.19 20 1 0 4 2 ## 9 22.8 4 141. 95 3.92 3.15 22.9 1 0 4 2 ## 10 19.2 6 168. 123 3.92 3.44 18.3 1 0 4 4 ## # … with 22 more rows  In summary, everything in the Second Edition will work just fine, but what you see in the Console might look a little different at times.\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"556d6b5c82b2f5857f44098c889f422a","permalink":"https://r4all.org/posts/importing-data-update/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/posts/importing-data-update/","section":"posts","summary":"As the Second Edition of Getting Started with R was going to press, Rstudio changed the function it uses to import data in the Import Dataset tool, from the base function read.csv() to the read_csv() function in the readr package. Since then, the Import Dataset button gives a menu with an option to use either (\u0026ldquo;base\u0026rdquo; uses read.csv and \u0026ldquo;readr\u0026rdquo; uses read_csv) From the Rstudio Blog about the readr package:","tags":[],"title":"Importing data update","type":"posts"},{"authors":null,"categories":null,"content":" Datasets used in Getting Started with R, Second Edition All the datasets (and more) one zip file.\nDatasets for the first edition of Getting Started with R are also contained in that zip file.\nDatasets used in Insights The datasets used in Insights are all available directly from publically accessible data repositories. Please see the book or companion website for details.\n","date":1536444000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1536444000,"objectID":"5c829381d43380f433483ca88f85e148","permalink":"https://r4all.org/books/datasets/","publishdate":"2018-09-09T00:00:00+02:00","relpermalink":"/books/datasets/","section":"books","summary":"Datasets used in Getting Started with R, Second Edition All the datasets (and more) one zip file.\nDatasets for the first edition of Getting Started with R are also contained in that zip file.\nDatasets used in Insights The datasets used in Insights are all available directly from publically accessible data repositories. Please see the book or companion website for details.","tags":null,"title":"Datasets","type":"docs"},{"authors":null,"categories":null,"content":"   We are pleased to announce the second edition of Getting Started with R Paperback at OUP, Amazon UK, Amazon.com, Amazon.de, Buch.ch\nElectric version at Oxford Scholarship Online.\nWe got great feedback about our book \u0026ldquo;Getting Started with R, An Introduction for Biologists\u0026rdquo;, but realised over the last couple of years that we were using and teaching R very differently from what we wrote about back in 2010. So we have almost completely rewritten for a very different and new Second Edition.\n Now based on using RStudio Now based around dplyr and ggplot2 for data management and grahics New basic examples for regression, 1-way ANOVA and 2-way ANOVA A whole new chapter on the Generalised Linear Model That same Getting Started with R attitude.  While many of the tools and functions have changed in this new edition, the content remains clear and enjoyable. We still walk your through, in that R4All style, from the basics of setting up your computer, through to data import and exploration and ultimately to a series of statistical models for interpreting data and experiments.\nWhat is so different and new? Several recent developments in the R community have made it even easier for us to help you focus more on your data and questions. RStudio has emerged as a brilliant cross-platform interface to working with R. Furthermore, Hadley Wickham and colleagues, have developed several add-on packages including as dplyr, tidyr and ggplot2 that not only provide consistent and intuitive ways to work with your data, but are emerging as industry standards (academic and non-academic).\nWe couldn\u0026rsquo;t afford to ignore these developments. The second edition thus dispenses entirely with classic methods of using R, and instead embraces RStudio along with dplyr, tidyr and ggplot2.\nYou\u0026rsquo;ll love it.\nAnything else new? We have a new co-author, Dylan Childs. He better at using R than Owen and Andrew, and kind of grumpy, so fits in well. He\u0026rsquo;s also helped us expand our stats offerings in the book. Not only do we provide more examples for basic stats (Regression, 1-way ANOVA, 2-way ANOVA) but a whole new chapter on the Generalised Linear Model.\n","date":1536444000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1536444000,"objectID":"7df86136d0d40f7e3353479e39cf7820","permalink":"https://r4all.org/books/gswr2/","publishdate":"2018-09-09T00:00:00+02:00","relpermalink":"/books/gswr2/","section":"books","summary":"We are pleased to announce the second edition of Getting Started with R Paperback at OUP, Amazon UK, Amazon.com, Amazon.de, Buch.ch\nElectric version at Oxford Scholarship Online.\nWe got great feedback about our book \u0026ldquo;Getting Started with R, An Introduction for Biologists\u0026rdquo;, but realised over the last couple of years that we were using and teaching R very differently from what we wrote about back in 2010. So we have almost completely rewritten for a very different and new Second Edition.","tags":null,"title":"Getting Started with R, Second Edition","type":"docs"},{"authors":null,"categories":null,"content":" Lost packages Here is a post Lost Packages about what to do if, when attempting to install a package you get a message like package ggfortify is not available (for R Version 3.2.4).\nImporting data update If you\u0026rsquo;re having a brain melt figuring out the difference between read_csv and read.csv and their use in the RStudio Import Dataset tool, look at this post about using the readr package and read_csv function to import data.\nAbout the first edition of Getting Started with R   “Blowing away any feeling of intimidation is what this book is about.” – Graeme Ruxton, Trends in Ecology Evolution.\nDesigned for undergraduates, grad students and staff, we show you how to import, explore, graph, and start analysing your data, keeping you focused on your ultimate goals.\n A simple, easy, and engaging introduction to R for biologists. Walks readers through the fundamentals of using R, from that first step of importing datathrough to managing and exploring it and ultimately producing figures and analyses Delivers an efficient, accurate, reliable, and reproducible workflow. We provide a simple, efficient, reliable, accurate, and reproducible workflow for you in an engaging and sometimes humorous format.  Getting Started with R – its the book you and your students want to read before all the others.\nWell, it was. Now you should only look at the Second Edition.\n","date":1536444000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1536444000,"objectID":"4eb2d8e54d17d72ffa553c8ce6589aac","permalink":"https://r4all.org/books/miscellany/","publishdate":"2018-09-09T00:00:00+02:00","relpermalink":"/books/miscellany/","section":"books","summary":"Lost packages Here is a post Lost Packages about what to do if, when attempting to install a package you get a message like package ggfortify is not available (for R Version 3.2.4).\nImporting data update If you\u0026rsquo;re having a brain melt figuring out the difference between read_csv and read.csv and their use in the RStudio Import Dataset tool, look at this post about using the readr package and read_csv function to import data.","tags":null,"title":"Miscellany","type":"docs"}]